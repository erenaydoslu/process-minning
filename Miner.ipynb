{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Group9's notebook with the code for DBL Process Mining Q3 2021.\n",
    "\n",
    "#### The notebook consists of several parts corresponding to the models/techniques we are using to predict the next events in their chains, as well as the times of finishing those events. These are:\n",
    "\n",
    "### 1) The baseline model\n",
    "\n",
    "### 1.5) The Triagram method\n",
    "\n",
    "(The Triagram method is not a model on its own but is capable of partial predictions by itself, so we will be using it to enchance the other prediction methods)\n",
    "\n",
    "### 2) The cluster-tree model\n",
    "\n",
    "### 3) The Markov chain model\n",
    "\n",
    "#### Before these, we will dedicate some code to load the data and split it into a training, validation and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual and the models' calls in a nutshell:\n",
    "\n",
    "### 1) The baseline:\n",
    "\n",
    "Call with run_baseline(df, df_train, df_test). If (full) df provided, predictions are done on the entire data plugged in and this entire dataset is returned. If df_train and df_test provided (split before using), model is trained on the train data and predictions are done on test data. Test dataset returned only.\n",
    "\n",
    "### 2) The cluster-tree model:\n",
    "\n",
    "Call with run_cluster_alg(df_train, df_test). Automatically uses the triagram within it (?). Test dataset returned only.\n",
    "\n",
    "### 3) Markov Chain model:\n",
    "\n",
    "Call with run_markov_alg(df_train, df_validation). Automatically uses triagram within it (?). Validation dataset returned only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import datetime\n",
    "import random\n",
    "import time\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn import preprocessing as pre\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import math\n",
    "import tracemalloc\n",
    "import psutil\n",
    "from sklearn.decomposition import PCA\n",
    "from random import randint\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are loading .csv datasets here. They were converted by us from the given .xes extension. See how was it done at the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading:\n",
    "\n",
    "Loads the data with some safety-features for common mishaps we had"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_time(time):\n",
    "    return (datetime.datetime.fromisoformat(time))\n",
    "\n",
    "def load_data(BPI = 'BPI.csv', BPI_attr = 'BPI_attr.csv',  data2012 = False, sample=False):\n",
    "    df_BPI = pd.read_csv(BPI)\n",
    "    df_BPI_attr = pd.read_csv(BPI_attr)\n",
    "    \n",
    "    if 'Unnamed: 0' in df_BPI.columns:\n",
    "        df_BPI = df_BPI.rename(columns={'Unnamed: 0': 'case_id', 'Unnamed: 1': 'step_number'})\n",
    "        df_BPI_attr = df_BPI_attr.rename(columns={'Unnamed: 0': 'case_id'})\n",
    "    \n",
    "    df_BPI['time:timestamp'] = df_BPI['time:timestamp'].apply(fix_time)\n",
    "\n",
    "    df_BPI_attr['REG_DATE'] = df_BPI_attr['REG_DATE'].apply(fix_time)\n",
    "    \n",
    "    df_BPI['time:weekday'] = [x.weekday() for x in df_BPI['time:timestamp']]\n",
    "    df_BPI['time:hour'] = [x.hour for x in df_BPI['time:timestamp']]\n",
    "    df_BPI['time:day'] = [str(x.day) + '-' + str(x.month) + '-' + str(x.year) for x in df_BPI['time:timestamp']]\n",
    "    df_BPI['time:event_count'] = df_BPI.groupby('time:day')['time:day'].transform('count')\n",
    "    df_BPI['time:busy_day'] = df_BPI['time:event_count'] > df_BPI['time:event_count'].describe()[6]\n",
    "    \n",
    "    if(sample):\n",
    "        df_BPI, df_BPI_attr = df_BPI[:50000], df_BPI_attr[:2359]\n",
    "    \n",
    "    return (df_BPI, df_BPI_attr)\n",
    "\n",
    "def load_data_xes(data):\n",
    "    BPI = xes_importer.apply(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Warning: if sample=True, please bear it in mind. If you have any loops iterating over the entire df, don't rig them\n",
    "df, df_attr = load_data(BPI = 'Datasets/BPI_2012.csv', BPI_attr = 'Datasets/BPI_attr_2012.csv', sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(df : pd.DataFrame, amount_train_data = 0.8, valitest_size = 0.2) -> tuple:\n",
    "    '''Plug in the df and train and validation data percentages\n",
    "    \n",
    "    the amount of train data splits the data in test and train/validation data\n",
    "    The valitest_size determines how much of the train/validation set in validation'''\n",
    "    \n",
    "    #Renaming the column names if neccesary and creating a copy of the input df:\n",
    "    splitData_df = df.rename(columns={'Unnamed: 0': 'case_id', 'Unnamed: 1': 'step_number'})\n",
    "    splitData_df_attr = df_attr.rename(columns={'Unnamed: 0': 'case_id'})\n",
    "    \n",
    "    #get the limit date\n",
    "    last_case_id = list(splitData_df['case_id'])[-1]\n",
    "    splitData_limit_date = splitData_df[splitData_df['case_id'] == round(last_case_id * amount_train_data)]\n",
    "    splitData_limit_date = list(splitData_limit_date['time:timestamp'])[0]\n",
    "\n",
    "    #divide the dateset into before and after the limit date\n",
    "    before_limit_date = splitData_df[splitData_df['time:timestamp'] < splitData_limit_date]\n",
    "    after_limit_date = splitData_df[splitData_df['time:timestamp'] >= splitData_limit_date]\n",
    "    \n",
    "    #get the test datasets\n",
    "    test_data_cases = list(after_limit_date[after_limit_date['step_number'] == 0]['case_id'])\n",
    "    df_test = splitData_df[splitData_df['case_id'].isin(test_data_cases)]\n",
    "\n",
    "    #get the cases for the train and validation data set\n",
    "    final_step_before_limit_date = pd.Series(before_limit_date.step_number.values,index=before_limit_date.case_id).to_dict()\n",
    "    final_steps = pd.Series(splitData_df.step_number.values,index=splitData_df.case_id).to_dict()\n",
    "    train_data_cases = [k for k in final_step_before_limit_date if k in final_steps and final_steps[k] == final_step_before_limit_date[k]]\n",
    "\n",
    "    #randomly divide up the train/validaiton data sets\n",
    "    mask = len(train_data_cases) * valitest_size\n",
    "    random.shuffle(train_data_cases)\n",
    "    df_validation = splitData_df[splitData_df['case_id'].isin(train_data_cases[:int(mask)])]\n",
    "    df_train = splitData_df[splitData_df['case_id'].isin(train_data_cases[int(mask):])]\n",
    "    \n",
    "    #Re-setting the index:\n",
    "    df_train, df_validation, df_test = df_train.reset_index(drop=True), df_validation.reset_index(drop=True), df_test.reset_index(drop=True)\n",
    "    \n",
    "    return(df_train, df_validation, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the sample data for this split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142631 34767 47823\n"
     ]
    }
   ],
   "source": [
    "df_train, df_validation, df_test = data_split(df, 0.8, 0.2)\n",
    "\n",
    "#Checking the sizes of the partial datasets:\n",
    "print(df_train.shape[0], df_validation.shape[0], df_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline:\n",
    "\n",
    "### Pre-processing part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_time_difference(df : pd.DataFrame):\n",
    "    '''Set the time difference column\n",
    "    This function is places here because of the erased non-complete actions'''\n",
    "    \n",
    "    df['time:time_between'] = df['time:timestamp'].diff()\n",
    "    df.loc[df['step_number'] == 0, 'time:time_between'] = pd.Timedelta(0)\n",
    "    df.loc[0, 'time:time_between'] = pd.Timedelta(0)\n",
    "    \n",
    "    df[\"time:time_between_seconds\"] = [int(x.total_seconds()) for x in df[\"time:time_between\"]]\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "def add_extra_event_columns(df : pd.DataFrame):\n",
    "    '''Combine lifecycle:transition and concept:name'''\n",
    "    \n",
    "    df['combined_names'] = df['lifecycle:transition'] + ' + ' + df['concept:name']\n",
    "    \n",
    "    df[\"next_event\"] = df[\"combined_names\"]\n",
    "    df.loc[df['step_number'] == 0, 'next_event'] = 'editor: close_case'\n",
    "    df[\"next_event\"] = df[\"next_event\"].shift(-1)\n",
    "    df.loc[len(df) - 1, 'next_event'] = 'editor: close_case'\n",
    "    \n",
    "    df[\"time:time_between\"] = df[\"time:timestamp\"].diff()\n",
    "    df.loc[df['step_number'] == 0, 'time:time_between'] = pd.Timedelta(0) #Changed it a bit to always insert 0 into the first row\n",
    "    df[\"time:time_between\"] = [int(x.total_seconds()) for x in df[\"time:time_between\"]]\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "def preprocessing(df):\n",
    "    '''runs the above 2 functions'''\n",
    "    \n",
    "    df = add_extra_event_columns(df)\n",
    "    df = compute_time_difference(df)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "\n",
    "### The algorithm's functions (only run 'run_baseline', it has the other ones embedded)\n",
    "\n",
    "# GUYS I COMMENTED OUT THE TRIAGRAM PART, SINCE WE WERE TOLD TO RATHER NOT USE IT HERE. THIS WAY WE CAN EASILY REVERT THE CHANGES IF NEEDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_dict_for_next_step_stats (df : pd.DataFrame, concept_name : str) -> dict:\n",
    "    '''For an input action checks for all the possible next actions and counts their occurence'''\n",
    "    df_concept = df[df['combined_names'] == concept_name]\n",
    "    \n",
    "    return df_concept['next_event'].mode().iloc[0], pd.to_timedelta(str(df_concept['time:time_between_seconds'].mean()) + 's')\n",
    "\n",
    "\n",
    "def add_expected_events(df : pd.DataFrame) -> list:\n",
    "    all_events = df['combined_names'].unique()\n",
    "    next_event_name_dic = {'editor: close_case': 'editor: close_case'}\n",
    "    next_event_duration_dic = {'editor: close_case': pd.Timedelta(0)}\n",
    "    \n",
    "    for event in all_events:\n",
    "        next_event_name_dic[event], next_event_duration_dic[event] = creating_dict_for_next_step_stats(df, event)\n",
    "    \n",
    "    return (next_event_name_dic, next_event_duration_dic)\n",
    "\n",
    "\n",
    "def baseline_on_single_df(baseline_df : pd.DataFrame, use_triagram = False, save_to_csv = False) -> pd.DataFrame:\n",
    "    '''Runs all the necessary functions to add the predictions to the input dataframe\n",
    "    \n",
    "    Where is the longest trace accounted for?'''\n",
    "    \n",
    "    df = baseline_df.copy()\n",
    "    #Adding next expected events:\n",
    "    all_expected_events = add_expected_events(df)\n",
    "    df['expect:next_event'] = df['combined_names'].map(all_expected_events[0])\n",
    "    \n",
    "    #Adding next expected times:\n",
    "    loop_max = df.shape[0]\n",
    "    next_time = [0] * loop_max\n",
    "    mapa = df['combined_names'].map(all_expected_events[1])\n",
    "    for i in tqdm(range(0, loop_max)):\n",
    "        next_time[i] = mapa[i] + df['time:timestamp'][i]\n",
    "    \n",
    "    df['expect:next_time'] = next_time\n",
    "    \n",
    "#     #applies triagram method\n",
    "#     df = apply_triagram(df)\n",
    "#     df.loc[df['combined P'].isnull(),'combined P'] = df['expect:next_time']\n",
    "#     df['expect:next_time'] = df['combined P']\n",
    "#     df = df.drop(['combined P', 'lifecycle:transition P', 'concept:name P'], axis=1)\n",
    "    \n",
    "    #Saving the results to a csv\n",
    "    if (save_to_csv):\n",
    "        df.to_csv('BPI_with_predictions.csv')\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "\n",
    "def baseline_on_train_test(df_train, df_test, use_triagram = False, save_to_csv = False) -> pd.DataFrame:\n",
    "    '''Runs all the necessary functions to add the predictions to the input dataframe\n",
    "    \n",
    "    Where is the longest trace accounted for?'''\n",
    "    \n",
    "    #Adding next expected events:\n",
    "    all_expected_events = add_expected_events(df_train) #Training on the train data\n",
    "    df_test['expect:next_event'] = df_test['combined_names'].map(all_expected_events[0])\n",
    "    \n",
    "    #Adding next expected times:\n",
    "    loop_max = df_test.shape[0]\n",
    "    next_time = [0] * loop_max\n",
    "    mapa = df_test['combined_names'].map(all_expected_events[1])\n",
    "    for i in tqdm(range(0, loop_max)):\n",
    "        next_time[i] = mapa[i] + df_test['time:timestamp'][i]\n",
    "    \n",
    "    df_test['expect:next_time'] = next_time\n",
    "    \n",
    "#     #Applying the triagram method:\n",
    "#     df_test = apply_triagram(df_test)\n",
    "#     df_test.loc[df_test['combined P'].isnull(),'combined P'] = df_test['expect:next_time']\n",
    "#     df_test['expect:next_time'] = df_test['combined P']\n",
    "#     df_test = df_test.drop(['combined P', 'lifecycle:transition P', 'concept:name P'], axis=1)\n",
    "    \n",
    "    #Saving the results to a csv\n",
    "    if (save_to_csv):\n",
    "        df_test.to_csv('BPI_with_predictions_test.csv')\n",
    "    \n",
    "    return(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "\n",
    "### Executions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baseline(df=df, df_train = [], df_test= [], use_triagram=False):  \n",
    "    '''Runnable on a single dataset - do not put any other datasets as parameters then (or pass no arguments)\n",
    "    If you wish to have the baseline ran on a train and test dataset (validation does not make much sense), plug them in'''\n",
    "    \n",
    "#     if(df_train == None and df_test == None):\n",
    "#         baseline_df = df.copy()\n",
    "\n",
    "    tracemalloc.start()\n",
    "    \n",
    "    #Will only take one short loading slider to load regardless of input type\n",
    "    if (len(df_train) == 0 and len(df_test) == 0): \n",
    "        print(\"Using the entire dataset. Full dataset is returned\")\n",
    "        baseline_df = baseline_on_single_df(df, save_to_csv=False) \n",
    "    else:\n",
    "        print(\"Using the split dataset. df_train is returned\")\n",
    "        baseline_df = baseline_on_train_test(df_train, df_test, save_to_csv=False)\n",
    "        \n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    print(f\"Current memory usage is {current / 10**6}MB; Peak was {peak / 10**6}MB\")\n",
    "    tracemalloc.stop()\n",
    "    cpu = psutil.cpu_percent()\n",
    "    print('Current CPU usage is {} %'.format(psutil.cpu_percent()))\n",
    "    \n",
    "    return (baseline_df) #Returns either the full dataset or just the test dataset, whatever you plug in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_train, df_validation, df_test = preprocessing(df), preprocessing(df_train), preprocessing(df_validation), preprocessing(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the split dataset. df_train is returned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9956/9956 [00:01<00:00, 5338.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory usage is 2.239192MB; Peak was 2.815989MB\n",
      "Current CPU usage is 0.0 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>step_number</th>\n",
       "      <th>org:resource</th>\n",
       "      <th>lifecycle:transition</th>\n",
       "      <th>concept:name</th>\n",
       "      <th>time:timestamp</th>\n",
       "      <th>time:weekday</th>\n",
       "      <th>time:hour</th>\n",
       "      <th>time:day</th>\n",
       "      <th>time:event_count</th>\n",
       "      <th>time:busy_day</th>\n",
       "      <th>combined_names</th>\n",
       "      <th>next_event</th>\n",
       "      <th>time:time_between</th>\n",
       "      <th>time:time_between_seconds</th>\n",
       "      <th>expect:next_event</th>\n",
       "      <th>expect:next_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1886</td>\n",
       "      <td>0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>A_SUBMITTED</td>\n",
       "      <td>2011-10-25 09:28:32.077000+02:00</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>25-10-2011</td>\n",
       "      <td>1864</td>\n",
       "      <td>False</td>\n",
       "      <td>COMPLETE + A_SUBMITTED</td>\n",
       "      <td>COMPLETE + A_PARTLYSUBMITTED</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETE + A_PARTLYSUBMITTED</td>\n",
       "      <td>2011-10-25 09:28:32.077000+02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1886</td>\n",
       "      <td>1</td>\n",
       "      <td>112.0</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>A_PARTLYSUBMITTED</td>\n",
       "      <td>2011-10-25 09:28:32.583000+02:00</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>25-10-2011</td>\n",
       "      <td>1864</td>\n",
       "      <td>False</td>\n",
       "      <td>COMPLETE + A_PARTLYSUBMITTED</td>\n",
       "      <td>SCHEDULE + W_Afhandelen leads</td>\n",
       "      <td>00:00:00.506000</td>\n",
       "      <td>0</td>\n",
       "      <td>SCHEDULE + W_Afhandelen leads</td>\n",
       "      <td>2011-10-25 09:28:32.762718876+02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1886</td>\n",
       "      <td>2</td>\n",
       "      <td>112.0</td>\n",
       "      <td>SCHEDULE</td>\n",
       "      <td>W_Afhandelen leads</td>\n",
       "      <td>2011-10-25 09:29:17.043000+02:00</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>25-10-2011</td>\n",
       "      <td>1864</td>\n",
       "      <td>False</td>\n",
       "      <td>SCHEDULE + W_Afhandelen leads</td>\n",
       "      <td>START + W_Afhandelen leads</td>\n",
       "      <td>00:00:44.460000</td>\n",
       "      <td>44</td>\n",
       "      <td>START + W_Afhandelen leads</td>\n",
       "      <td>2011-10-25 09:29:46.695061856+02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1886</td>\n",
       "      <td>3</td>\n",
       "      <td>10889.0</td>\n",
       "      <td>START</td>\n",
       "      <td>W_Afhandelen leads</td>\n",
       "      <td>2011-10-25 10:01:54.255000+02:00</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>25-10-2011</td>\n",
       "      <td>1864</td>\n",
       "      <td>False</td>\n",
       "      <td>START + W_Afhandelen leads</td>\n",
       "      <td>COMPLETE + A_DECLINED</td>\n",
       "      <td>00:32:37.212000</td>\n",
       "      <td>1957</td>\n",
       "      <td>COMPLETE + A_DECLINED</td>\n",
       "      <td>2011-10-25 14:30:55.175668058+02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1886</td>\n",
       "      <td>4</td>\n",
       "      <td>10889.0</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>A_DECLINED</td>\n",
       "      <td>2011-10-25 10:02:12.465000+02:00</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>25-10-2011</td>\n",
       "      <td>1864</td>\n",
       "      <td>False</td>\n",
       "      <td>COMPLETE + A_DECLINED</td>\n",
       "      <td>COMPLETE + W_Afhandelen leads</td>\n",
       "      <td>00:00:18.210000</td>\n",
       "      <td>18</td>\n",
       "      <td>editor: close_case</td>\n",
       "      <td>2011-10-25 10:05:19.873031088+02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9951</td>\n",
       "      <td>2358</td>\n",
       "      <td>25</td>\n",
       "      <td>11181.0</td>\n",
       "      <td>START</td>\n",
       "      <td>W_Completeren aanvraag</td>\n",
       "      <td>2011-11-07 17:19:57.689000+01:00</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>7-11-2011</td>\n",
       "      <td>2453</td>\n",
       "      <td>False</td>\n",
       "      <td>START + W_Completeren aanvraag</td>\n",
       "      <td>COMPLETE + W_Completeren aanvraag</td>\n",
       "      <td>00:07:57.800000</td>\n",
       "      <td>477</td>\n",
       "      <td>COMPLETE + W_Completeren aanvraag</td>\n",
       "      <td>2011-11-07 23:15:47.667615071+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9952</td>\n",
       "      <td>2358</td>\n",
       "      <td>26</td>\n",
       "      <td>11181.0</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>W_Completeren aanvraag</td>\n",
       "      <td>2011-11-07 17:21:37.466000+01:00</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>7-11-2011</td>\n",
       "      <td>2453</td>\n",
       "      <td>False</td>\n",
       "      <td>COMPLETE + W_Completeren aanvraag</td>\n",
       "      <td>START + W_Completeren aanvraag</td>\n",
       "      <td>00:01:39.777000</td>\n",
       "      <td>99</td>\n",
       "      <td>START + W_Completeren aanvraag</td>\n",
       "      <td>2011-11-07 17:24:53.795603255+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9953</td>\n",
       "      <td>2358</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>START</td>\n",
       "      <td>W_Completeren aanvraag</td>\n",
       "      <td>2011-11-08 10:39:15.079000+01:00</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>8-11-2011</td>\n",
       "      <td>2309</td>\n",
       "      <td>False</td>\n",
       "      <td>START + W_Completeren aanvraag</td>\n",
       "      <td>COMPLETE + W_Completeren aanvraag</td>\n",
       "      <td>17:17:37.613000</td>\n",
       "      <td>62257</td>\n",
       "      <td>COMPLETE + W_Completeren aanvraag</td>\n",
       "      <td>2011-11-08 16:35:05.057615071+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9954</td>\n",
       "      <td>2358</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>W_Completeren aanvraag</td>\n",
       "      <td>2011-11-08 10:40:29.255000+01:00</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>8-11-2011</td>\n",
       "      <td>2309</td>\n",
       "      <td>False</td>\n",
       "      <td>COMPLETE + W_Completeren aanvraag</td>\n",
       "      <td>START + W_Completeren aanvraag</td>\n",
       "      <td>00:01:14.176000</td>\n",
       "      <td>74</td>\n",
       "      <td>START + W_Completeren aanvraag</td>\n",
       "      <td>2011-11-08 10:43:45.584603255+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9955</td>\n",
       "      <td>2358</td>\n",
       "      <td>29</td>\n",
       "      <td>11181.0</td>\n",
       "      <td>START</td>\n",
       "      <td>W_Completeren aanvraag</td>\n",
       "      <td>2011-11-08 18:43:29.748000+01:00</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>8-11-2011</td>\n",
       "      <td>2309</td>\n",
       "      <td>False</td>\n",
       "      <td>START + W_Completeren aanvraag</td>\n",
       "      <td>editor: close_case</td>\n",
       "      <td>08:03:00.493000</td>\n",
       "      <td>28980</td>\n",
       "      <td>COMPLETE + W_Completeren aanvraag</td>\n",
       "      <td>2011-11-09 00:39:19.726615071+01:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9956 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      case_id  step_number  org:resource lifecycle:transition  \\\n",
       "0        1886            0         112.0             COMPLETE   \n",
       "1        1886            1         112.0             COMPLETE   \n",
       "2        1886            2         112.0             SCHEDULE   \n",
       "3        1886            3       10889.0                START   \n",
       "4        1886            4       10889.0             COMPLETE   \n",
       "...       ...          ...           ...                  ...   \n",
       "9951     2358           25       11181.0                START   \n",
       "9952     2358           26       11181.0             COMPLETE   \n",
       "9953     2358           27           NaN                START   \n",
       "9954     2358           28           NaN             COMPLETE   \n",
       "9955     2358           29       11181.0                START   \n",
       "\n",
       "                concept:name                    time:timestamp  time:weekday  \\\n",
       "0                A_SUBMITTED  2011-10-25 09:28:32.077000+02:00             1   \n",
       "1          A_PARTLYSUBMITTED  2011-10-25 09:28:32.583000+02:00             1   \n",
       "2         W_Afhandelen leads  2011-10-25 09:29:17.043000+02:00             1   \n",
       "3         W_Afhandelen leads  2011-10-25 10:01:54.255000+02:00             1   \n",
       "4                 A_DECLINED  2011-10-25 10:02:12.465000+02:00             1   \n",
       "...                      ...                               ...           ...   \n",
       "9951  W_Completeren aanvraag  2011-11-07 17:19:57.689000+01:00             0   \n",
       "9952  W_Completeren aanvraag  2011-11-07 17:21:37.466000+01:00             0   \n",
       "9953  W_Completeren aanvraag  2011-11-08 10:39:15.079000+01:00             1   \n",
       "9954  W_Completeren aanvraag  2011-11-08 10:40:29.255000+01:00             1   \n",
       "9955  W_Completeren aanvraag  2011-11-08 18:43:29.748000+01:00             1   \n",
       "\n",
       "      time:hour    time:day  time:event_count  time:busy_day  \\\n",
       "0             9  25-10-2011              1864          False   \n",
       "1             9  25-10-2011              1864          False   \n",
       "2             9  25-10-2011              1864          False   \n",
       "3            10  25-10-2011              1864          False   \n",
       "4            10  25-10-2011              1864          False   \n",
       "...         ...         ...               ...            ...   \n",
       "9951         17   7-11-2011              2453          False   \n",
       "9952         17   7-11-2011              2453          False   \n",
       "9953         10   8-11-2011              2309          False   \n",
       "9954         10   8-11-2011              2309          False   \n",
       "9955         18   8-11-2011              2309          False   \n",
       "\n",
       "                         combined_names                         next_event  \\\n",
       "0                COMPLETE + A_SUBMITTED       COMPLETE + A_PARTLYSUBMITTED   \n",
       "1          COMPLETE + A_PARTLYSUBMITTED      SCHEDULE + W_Afhandelen leads   \n",
       "2         SCHEDULE + W_Afhandelen leads         START + W_Afhandelen leads   \n",
       "3            START + W_Afhandelen leads              COMPLETE + A_DECLINED   \n",
       "4                 COMPLETE + A_DECLINED      COMPLETE + W_Afhandelen leads   \n",
       "...                                 ...                                ...   \n",
       "9951     START + W_Completeren aanvraag  COMPLETE + W_Completeren aanvraag   \n",
       "9952  COMPLETE + W_Completeren aanvraag     START + W_Completeren aanvraag   \n",
       "9953     START + W_Completeren aanvraag  COMPLETE + W_Completeren aanvraag   \n",
       "9954  COMPLETE + W_Completeren aanvraag     START + W_Completeren aanvraag   \n",
       "9955     START + W_Completeren aanvraag                 editor: close_case   \n",
       "\n",
       "     time:time_between  time:time_between_seconds  \\\n",
       "0             00:00:00                          0   \n",
       "1      00:00:00.506000                          0   \n",
       "2      00:00:44.460000                         44   \n",
       "3      00:32:37.212000                       1957   \n",
       "4      00:00:18.210000                         18   \n",
       "...                ...                        ...   \n",
       "9951   00:07:57.800000                        477   \n",
       "9952   00:01:39.777000                         99   \n",
       "9953   17:17:37.613000                      62257   \n",
       "9954   00:01:14.176000                         74   \n",
       "9955   08:03:00.493000                      28980   \n",
       "\n",
       "                      expect:next_event                     expect:next_time  \n",
       "0          COMPLETE + A_PARTLYSUBMITTED     2011-10-25 09:28:32.077000+02:00  \n",
       "1         SCHEDULE + W_Afhandelen leads  2011-10-25 09:28:32.762718876+02:00  \n",
       "2            START + W_Afhandelen leads  2011-10-25 09:29:46.695061856+02:00  \n",
       "3                 COMPLETE + A_DECLINED  2011-10-25 14:30:55.175668058+02:00  \n",
       "4                    editor: close_case  2011-10-25 10:05:19.873031088+02:00  \n",
       "...                                 ...                                  ...  \n",
       "9951  COMPLETE + W_Completeren aanvraag  2011-11-07 23:15:47.667615071+01:00  \n",
       "9952     START + W_Completeren aanvraag  2011-11-07 17:24:53.795603255+01:00  \n",
       "9953  COMPLETE + W_Completeren aanvraag  2011-11-08 16:35:05.057615071+01:00  \n",
       "9954     START + W_Completeren aanvraag  2011-11-08 10:43:45.584603255+01:00  \n",
       "9955  COMPLETE + W_Completeren aanvraag  2011-11-09 00:39:19.726615071+01:00  \n",
       "\n",
       "[9956 rows x 17 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_results = run_baseline(df_train=df_train, df_test=df_test)\n",
    "baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "\n",
    "### Assessing its accuracy. This is also the function to check the accuracy of each model in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 65.4%\n"
     ]
    }
   ],
   "source": [
    "#prints accuracy of the predictions on the dataset\n",
    "def accuracy_assess(df_results, col_name = 'expect:next_event'):\n",
    "    accuracy = round((len(df_results[df_results['next_event'] == df_results[col_name]])/len(df_results))*100, 1)\n",
    "    print('accuracy: ' + str(accuracy) + '%')\n",
    "    \n",
    "accuracy_assess(baseline_results, col_name = 'expect:next_event')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triagram method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_triagram(df : pd.DataFrame, lifecycle_name = 'lifecycle:transition P', concept_name = 'concept:name P', combined_name = 'combined P') -> pd.DataFrame:\n",
    "    '''creates column lifecycle_name (called 'lifecycle:transition P' by default)\n",
    "    and the column concept_name (called 'concept:name P' by default), which contains the prediction we know for certain.\n",
    "    If a value can not be predicted like this it is given the np.nan value'''\n",
    "    \n",
    "    #creates some temporary shifted columns\n",
    "    df['concept:name S1'] = df['concept:name'].shift(1)\n",
    "    df['concept:name S2'] = df['concept:name'].shift(2)\n",
    "    df['lifecycle:transition S1'] = df['lifecycle:transition'].shift(1)\n",
    "    df['lifecycle:transition S2'] = df['lifecycle:transition'].shift(2)\n",
    "    df['case_id S1'] = df['case_id'].shift(1)\n",
    "    df['case_id S2'] = df['case_id'].shift(2)\n",
    "\n",
    "    #gets dataframe with all first and a dataframe with all second cases\n",
    "    first_case = df['step_number'] == 0\n",
    "    second_case = df['step_number'] == 1\n",
    "    \n",
    "    #gets 5 different sets, all of which test a different condition\n",
    "    triagram1 = df['lifecycle:transition S1']+df['lifecycle:transition S2'] == 'COMPLETESCHEDULE'\n",
    "    triagram2 = df['concept:name S1']+df['concept:name S1'] == 'WW'\n",
    "    triagram3 = df['concept:name S1'] != df['concept:name S1']\n",
    "    triagram4 = df['case_id'] == df['case_id S1']\n",
    "    triagram5 = df['case_id'] == df['case_id S2']\n",
    "    \n",
    "    #creates the empty columns\n",
    "    df[lifecycle_name] = np.nan\n",
    "    df[concept_name] = np.nan\n",
    "    \n",
    "    #adds the lifecycle entries for the comfirmed cases\n",
    "    df.loc[first_case, lifecycle_name] = 'COMPLETE'\n",
    "    df.loc[second_case, lifecycle_name] = 'COMPLETE'\n",
    "    df.loc[(triagram1&triagram2&triagram3&triagram4&triagram5), 'concept:name P'] = 'START'\n",
    "    \n",
    "    #add the concept name entries for the comfirmed cases\n",
    "    df.loc[first_case, concept_name] = 'A_SUBMITTED'\n",
    "    df.loc[second_case, concept_name] = 'A_PARTLYSUBMITTED'\n",
    "    df.loc[(triagram1&triagram2&triagram3&triagram4&triagram5), 'concept:name P'] = df.loc[(triagram1&triagram2&triagram3&triagram4&triagram5), 'concept:name S2']\n",
    "    \n",
    "    #drop all unneaded columns and adds the combined column\n",
    "    df = df.drop(['concept:name S1', 'concept:name S2', 'lifecycle:transition S1', 'lifecycle:transition S2', 'case_id S1', 'case_id S2'], axis=1)\n",
    "    df[lifecycle_name] = df[lifecycle_name].shift(-1)\n",
    "    df[concept_name] = df[concept_name].shift(-1)\n",
    "    df[combined_name] = df[lifecycle_name] + ' + ' + df[concept_name]\n",
    "    df[combined_name] = df[combined_name].map({'COMPLETE + A_SUBMITTED': 'editor: close_case'})\n",
    "    return (df)\n",
    "\n",
    "\n",
    "def add_W_active(df, column_name = 'W_active'):\n",
    "    '''creates new column (called W_active by default), which tracks if a W-event is active before that event occurs'''\n",
    "    \n",
    "    converter = {'STARTW':True, 'COMPLETEW': False}\n",
    "    df['W_active'] = False\n",
    "    df['life_concept'] = df['lifecycle:transition'] + df['concept:name'].astype(str).str[0]\n",
    "    df['life_concept S1'] = df['life_concept'].shift(1)\n",
    "\n",
    "    df.loc[0, 'W_active'] = False\n",
    "    df.loc[df['step_number'] ==0, 'W_active'] = False\n",
    "    df.loc[df['life_concept S1'] == 'STARTW', 'W_active'] = True\n",
    "    df.loc[df['life_concept S1'] == 'COMPLETEW', 'W_active'] = False\n",
    "    \n",
    "    for i in range(0, 50):  \n",
    "        df['W_active'] = df['W_active'].shift(1)\n",
    "    \n",
    "        df.loc[0, 'W_active'] = False\n",
    "        df.loc[df['step_number'] ==0, 'W_active'] = False\n",
    "        df.loc[df['life_concept S1'] == 'STARTW', 'W_active'] = True\n",
    "        df.loc[df['life_concept S1'] == 'COMPLETEW', 'W_active'] = False\n",
    "\n",
    "    df = df.drop(columns = ['life_concept', 'life_concept S1'])\n",
    "    return (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our dataset is loaded and split at this point\n",
    "df = apply_triagram(df)\n",
    "df = add_W_active(df)\n",
    "\n",
    "#Running the apply_trigram() and add_W_active() functions on the entire split data:\n",
    "df_train, df_validation, df_test = apply_triagram(df_train), apply_triagram(df_validation), apply_triagram(df_test)\n",
    "df_train, df_validation, df_test = add_W_active(df_train), add_W_active(df_validation), add_W_active(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing the accuracy of predictions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple function to print out some statistics of a model (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timely_statistics(df_tree : pd.DataFrame, relative : bool, statistic : str) -> None:\n",
    "    '''Possibly invoke with:\n",
    "    for element in [(False, 'mean'), (False, 'median'), (True, 'mean'), (True, 'median')]:\n",
    "        timely_statistics(df, relative=element[0], statistic=element[1])'''\n",
    "    \n",
    "    if(statistic == 'mean' and relative == False):\n",
    "        seconds1, seconds2 = df_tree['time:absolute_prediction_off'].mean(), df_tree['time:absolute_prediction_correct'].mean()\n",
    "        minutes1, hours1, days1, minutes2, hours2, days2 = seconds1/60, seconds1/3600, seconds1/86400, seconds2/60, seconds2/3600, seconds2/86400\n",
    "        print('absolute off mean: ' + str(seconds1) + ', ' + str(minutes1) + ', ' + str(hours1) + ', ' + str(days1))\n",
    "        print('absolute correct mean: ' + str(seconds2) + ', ' + str(minutes2) + ', ' + str(hours2) + ', ' + str(days2))\n",
    "        \n",
    "    if(statistic == 'median' and relative == False):\n",
    "        seconds1, seconds2 = df_tree['time:absolute_prediction_off'].median(), df_tree['time:absolute_prediction_correct'].median()\n",
    "        minutes1, hours1, days1, minutes2, hours2, days2 = seconds1/60, seconds1/3600, seconds1/86400, seconds2/60, seconds2/3600, seconds2/86400\n",
    "        print('absolute off median: ' + str(seconds1) + ', ' + str(minutes1) + ', ' + str(hours1) + ', ' + str(days1))\n",
    "        print('absolute correct median: ' + str(seconds2) + ', ' + str(minutes2) + ', ' + str(hours2) + ', ' + str(days2))\n",
    "        \n",
    "    if(statistic == 'mean' and relative==True):\n",
    "        df_tree['time:absolute_prediction_off'].hist(bins=10)\n",
    "        seconds1, seconds2 = df_tree['time:relative_prediction_off'].mean(), df_tree['time:relative_prediction_correct'].mean()\n",
    "        minutes1, hours1, days1, minutes2, hours2, days2 = seconds1/60, seconds1/3600, seconds1/86400, seconds2/60, seconds2/3600, seconds2/86400\n",
    "        print('relative off mean: ' + str(seconds1) + ', ' + str(minutes1) + ', ' + str(hours1) + ', ' + str(days1))\n",
    "        print('relative correct mean: ' + str(seconds2) + ', ' + str(minutes2) + ', ' + str(hours2) + ', ' + str(days2))\n",
    "        \n",
    "    if(statistic == 'median' and relative==True):\n",
    "        df_tree['time:absolute_prediction_off'].hist(bins=10)\n",
    "        seconds1, seconds2 = df_tree['time:relative_prediction_off'].median(), df_tree['time:relative_prediction_correct'].median()\n",
    "        minutes1, hours1, days1, minutes2, hours2, days2 = seconds1/60, seconds1/3600, seconds1/86400, seconds2/60, seconds2/3600, seconds2/86400\n",
    "        print('relative off median: ' + str(seconds1) + ', ' + str(minutes1) + ', ' + str(hours1) + ', ' + str(days1))\n",
    "        print('relative correct median: ' + str(seconds2) + ', ' + str(minutes2) + ', ' + str(hours2) + ', ' + str(days2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster-Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe_for_clustering(raw_df):\n",
    "    ''''''\n",
    "    \n",
    "    df = raw_df.copy()\n",
    "    df['combined_names'] = df['lifecycle:transition'] + ' + ' + df['concept:name']\n",
    "    \n",
    "    #Prepare the time between columns\n",
    "    if (type(df['time:timestamp'].iloc[0]) != datetime.datetime):\n",
    "        df['time:timestamp'] = df['time:timestamp'].apply(fix_time)\n",
    "\n",
    "    df[\"time:time_between\"] = df[\"time:timestamp\"].diff()\n",
    "    df.loc[df['step_number'] == 0, 'time:time_between'] = pd.Timedelta(0) #Changed it a bit to always insert 0 into the first row\n",
    "    df[\"time:time_between\"] = [int(x.total_seconds()) for x in df[\"time:time_between\"]]\n",
    "    \n",
    "    #Prepare the next event column\n",
    "    df[\"next_event\"] = df[\"combined_names\"]\n",
    "    df.loc[df['step_number'] == 0, 'next_event'] = 'editor: close_case'\n",
    "    df[\"next_event\"] = df[\"next_event\"].shift(-1)\n",
    "    df.loc[len(df) - 1, 'next_event'] = 'editor: close_case'\n",
    "    \n",
    "    return (df)\n",
    "\n",
    "\n",
    "def prepare_column_for_clustering(case_id_column, cluster_column, unique_values):\n",
    "    ''''''\n",
    "    \n",
    "    grouped_df = pd.DataFrame({'case_id': case_id_column, 'column': cluster_column})\n",
    "    \n",
    "    for val in unique_values:\n",
    "        grouped_df[val] = 0\n",
    "        grouped_df.loc[grouped_df['column'] == val, val] = 1\n",
    "    \n",
    "    return grouped_df[['case_id'] + list(unique_values)]\n",
    "\n",
    "\n",
    "def get_clusters(prepared_rows):\n",
    "    ''''''\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=NR_OF_CLUSTERS)\n",
    "    \n",
    "    unique_columns = list(prepared_rows.columns)\n",
    "    unique_columns.pop(0)\n",
    "\n",
    "    df_grouped = prepared_rows.groupby('case_id')[unique_columns].sum()\n",
    "    kmeans.fit(df_grouped[unique_columns])\n",
    "    prediction = kmeans.labels_\n",
    "    unique_case_ids = prepared_rows['case_id'].unique()\n",
    "    return prepared_rows.case_id.map({unique_case_ids[i]: prediction[i] for i in range(len(prediction))}), kmeans\n",
    "\n",
    "\n",
    "def get_tree(df, clusters, MAX_DEPTH, N_ESTIMATORS):\n",
    "    ''''''\n",
    "    \n",
    "    model = MultiOutputClassifier(RandomForestClassifier(n_estimators=N_ESTIMATORS, max_depth=MAX_DEPTH))\n",
    "    \n",
    "    X_dtc = df[clusters + ['step_number', 'time:weekday', 'time:hour', 'lifecycle:transition int', 'W_active', 'time:busy_day']]\n",
    "    y_dtc = df[['next_event int', 'time:time_between']]\n",
    "\n",
    "    model.fit(X_dtc, y_dtc)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_clusters(cluster_model, prepared_rows, unique_columns):\n",
    "    ''''''\n",
    "    \n",
    "    df_grouped = prepared_rows.groupby('case_id')[unique_columns].sum()\n",
    "    prediction = cluster_model.predict(df_grouped[unique_columns])\n",
    "    unique_case_ids = prepared_rows['case_id'].unique()\n",
    "    return prepared_rows.case_id.map({unique_case_ids[i]: prediction[i] for i in range(len(prediction))})\n",
    "\n",
    "\n",
    "def predict_value(tree, df_predict, clusters):\n",
    "    ''''''\n",
    "    \n",
    "    X_dtc = df_predict[clusters + ['step_number', 'time:weekday', 'time:hour', 'lifecycle:transition int', 'W_active', 'time:busy_day']]\n",
    "    \n",
    "    prediction = tree.predict(X_dtc)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_tree_predict(df_train, df_test):\n",
    "    ''''''\n",
    "    \n",
    "    #I splitted this into 2 lines solely for debugging purposes:\n",
    "    cluster_tree_train = prepare_dataframe_for_clustering(df_train)\n",
    "    cluster_tree_test = prepare_dataframe_for_clustering(df_test)\n",
    "    \n",
    "    cluster_tree_train = add_W_active(cluster_tree_train)\n",
    "    cluster_tree_test = add_W_active(cluster_tree_test)\n",
    "    \n",
    "    le = pre.LabelEncoder()\n",
    "    le.fit(cluster_tree_train['lifecycle:transition'].unique())\n",
    "    cluster_tree_train['lifecycle:transition int'] = le.transform(cluster_tree_train['lifecycle:transition'])\n",
    "    cluster_tree_test['lifecycle:transition int'] = le.transform(cluster_tree_test['lifecycle:transition'])\n",
    "    \n",
    "    le2 = pre.LabelEncoder()\n",
    "    unique_ones = np.concatenate((cluster_tree_train['next_event'].unique(), cluster_tree_test['next_event'].unique()))\n",
    "    le2.fit(unique_ones)\n",
    "    cluster_tree_train['next_event int'] = le2.transform(cluster_tree_train['next_event'])\n",
    "    cluster_tree_test['next_event int'] = le2.transform(cluster_tree_test['next_event'])\n",
    "    \n",
    "    #Clustering\n",
    "    cluster_columns = ['combined_names'] # ENTER CLUSTER COLUMN NAMES (max 2) HERE\n",
    "    assigned_columns = [cluster_columns[i] + ' cluster' for i in range(len(cluster_columns))]\n",
    "    \n",
    "    for i in range(len(cluster_columns)):\n",
    "        #Make sure no NaN values exist in these columns\n",
    "        cluster_tree_train[cluster_columns[i]] = cluster_tree_train[cluster_columns[i]].fillna(-1)\n",
    "        cluster_tree_test[cluster_columns[i]] = cluster_tree_test[cluster_columns[i]].fillna(-1)\n",
    "        \n",
    "        unique_values = list(cluster_tree_train[cluster_columns[i]].unique())\n",
    "        unique_values_test = cluster_tree_test[cluster_columns[i]].unique()\n",
    "        \n",
    "        unique_values.extend(x for x in unique_values_test if x not in unique_values)\n",
    "        \n",
    "        prepared_train_data = prepare_column_for_clustering(cluster_tree_train['case_id'], cluster_tree_train[cluster_columns[i]], unique_values)\n",
    "        cluster_tree_train[assigned_columns[i]], cluster_model = get_clusters(prepared_train_data)\n",
    "\n",
    "        prepared_test_data = prepare_column_for_clustering(cluster_tree_test['case_id'], cluster_tree_test[cluster_columns[i]], unique_values)\n",
    "        cluster_tree_test[assigned_columns[i]] = predict_clusters(cluster_model, prepared_test_data, unique_values)\n",
    "    \n",
    "    first_steps_train = cluster_tree_train['step_number'] <= CLUSTER_STEP_SPLIT\n",
    "    first_steps_test = cluster_tree_test['step_number'] <= CLUSTER_STEP_SPLIT\n",
    "    \n",
    "    model = get_tree(cluster_tree_train[first_steps_train], ['next_event int'], BEGIN_TREE_DEPTH, N_ESTIMATORS)\n",
    "    result = predict_value(model, cluster_tree_test[first_steps_test], ['next_event int'])\n",
    "    cluster_tree_test.loc[first_steps_test, 'prediction'] = le2.inverse_transform(result[:, 0])\n",
    "    cluster_tree_test.loc[first_steps_test, 'time:between_predicted'] = result[:, 1]\n",
    "    \n",
    "    for i in tqdm(range(NR_OF_CLUSTERS)):\n",
    "        next_amount = NR_OF_CLUSTERS\n",
    "        if len(cluster_columns) == 1:\n",
    "            next_amount = 1\n",
    "        for j in range(next_amount):\n",
    "            first_cluster_train = (cluster_tree_train[assigned_columns[0]] == i)\n",
    "            first_cluster_test = (cluster_tree_test[assigned_columns[0]] == i) & (cluster_tree_test['step_number'] > CLUSTER_STEP_SPLIT)\n",
    "            \n",
    "            if len(cluster_columns) == 1:\n",
    "                clustered_train_df = cluster_tree_train[first_cluster_train]\n",
    "                clustered_test_df = cluster_tree_test[first_cluster_test]\n",
    "            else:\n",
    "                clustered_train_df = cluster_tree_train[first_cluster_train & (cluster_tree_test[assigned_columns[1]] == j)]\n",
    "                clustered_test_df = cluster_tree_test[first_cluster_test & (cluster_tree_test[assigned_columns[1]] == j)]\n",
    "        \n",
    "            if clustered_train_df.size > 0:\n",
    "                model = get_tree(clustered_train_df, assigned_columns, CLUSTER_TREE_DEPTH, N_ESTIMATORS)\n",
    "        \n",
    "                if clustered_test_df.size > 0:\n",
    "                    if len(cluster_columns) == 1:\n",
    "                        result = predict_value(model, clustered_test_df, assigned_columns)\n",
    "                        cluster_tree_test.loc[first_cluster_test, 'prediction'] = le2.inverse_transform(result[:, 0])\n",
    "                        cluster_tree_test.loc[first_cluster_test, 'time:between_predicted'] = result[:, 1]\n",
    "                    else:\n",
    "                        result = predict_value(model, clustered_test_df, assigned_columns)\n",
    "                        cluster_tree_test.loc[first_cluster_test & (cluster_tree_test[assigned_columns[1]] == j), 'prediction'] = le2.inverse_transform(result[:, 0])\n",
    "                        cluster_tree_test.loc[first_cluster_test & (cluster_tree_test[assigned_columns[1]] == j), 'time:between_predicted'] = result[:, 1]\n",
    "    \n",
    "    #apply the triagram method to the predictions\n",
    "    cluster_tree_test = apply_triagram(cluster_tree_test)\n",
    "    cluster_tree_test.loc[cluster_tree_test['combined P'].isnull(),'combined P'] = cluster_tree_test['prediction']\n",
    "    cluster_tree_test['prediction'] = cluster_tree_test['combined P']\n",
    "    cluster_tree_test = cluster_tree_test.drop(['combined P', 'lifecycle:transition int', 'next_event int', 'combined_names cluster', 'lifecycle:transition P', 'concept:name P'], axis=1)\n",
    "        \n",
    "    return cluster_tree_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model testing using test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this testing was done on the validation set\n",
    "NR_OF_CLUSTERS = 14\n",
    "CLUSTER_STEP_SPLIT, BEGIN_TREE_DEPTH, CLUSTER_TREE_DEPTH, N_ESTIMATORS = 7, 7, 12, 75\n",
    "\n",
    "# 7, 7, 12, 75 = 69.4%\n",
    "# 9, 5, 12, 75 = 69.1%\n",
    "# 6, 6, 10, 75 = 68.3\n",
    "# 6, 6, 8, 100 = 67.5%\n",
    "# 5, 5, 10, 100 = 67.3%\n",
    "# 7, 7, 15, 50 = 66.2%\n",
    "# 4, 4, 10, 125 = 66.2\n",
    "# 6, 4, 10, 100 = 66.0%\n",
    "# 5, 5, 7, 125 = 65.5%\n",
    "# 5, 3, 10, 125 = 64.5%\n",
    "# 5, 3, 7, 125 = 63.6%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cluster_alg(df_train, df_test):\n",
    "    ''''''\n",
    "    \n",
    "    tracemalloc.start()\n",
    "    df_tree_cluster = cluster_tree_predict(df_train, df_test) #Will only take one short loading slider to load\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    print(f\"Current memory usage is {current / 10**6}MB; Peak was {peak / 10**6}MB\")\n",
    "    tracemalloc.stop()\n",
    "    cpu = psutil.cpu_percent()\n",
    "    print('Current CPU usage is {} %'.format(psutil.cpu_percent()))\n",
    "    return (df_tree_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If we need to re-load the data before applying this technique - uncomment the next line. If not - delete it and this message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Reloading the data to revert the previous (triagram, preprocessing) changes:\n",
    "# df, df_attr = load_data(BPI = 'Datasets/BPI_2012.csv', BPI_attr = 'Datasets/BPI_attr_2012.csv', sample=False)\n",
    "# df_train, df_validation, df_test = data_split(df, 0.8, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same here if we need to re-apply the Triagram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Our dataset is loaded and split at this point\n",
    "# df = apply_triagram(df)\n",
    "# df = add_W_active(df)\n",
    "\n",
    "# #Running the apply_trigram() and add_W_active() functions on the entire split data:\n",
    "# df_train, df_validation, df_test = apply_triagram(df_train), apply_triagram(df_validation), apply_triagram(df_test)\n",
    "# df_train, df_validation, df_test = add_W_active(df_train), add_W_active(df_validation), add_W_active(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-0a4b60b23706>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#I used tree as a standin for the df_test result, this can be changed later\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_tree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_cluster_alg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf_tree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-92-89634e3c5fb7>\u001b[0m in \u001b[0;36mrun_cluster_alg\u001b[1;34m(df_train, df_test)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtracemalloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mdf_tree_cluster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcluster_tree_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Will only take one short loading slider to load\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mcurrent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpeak\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtracemalloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_traced_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Current memory usage is {current / 10**6}MB; Peak was {peak / 10**6}MB\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-90-f2a51edeac30>\u001b[0m in \u001b[0;36mcluster_tree_predict\u001b[1;34m(df_train, df_test)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mfirst_steps_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcluster_tree_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step_number'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mCLUSTER_STEP_SPLIT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcluster_tree_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfirst_steps_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'next_event int'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBEGIN_TREE_DEPTH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_ESTIMATORS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcluster_tree_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfirst_steps_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'next_event int'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0mcluster_tree_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfirst_steps_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'prediction'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mle2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-89-6a9718543e3f>\u001b[0m in \u001b[0;36mget_tree\u001b[1;34m(df, clusters, MAX_DEPTH, N_ESTIMATORS)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0my_dtc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'next_event int'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'time:time_between'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_dtc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_dtc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\AnacondaFolder\\lib\\site-packages\\sklearn\\multioutput.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    167\u001b[0m             delayed(_fit_estimator)(\n\u001b[0;32m    168\u001b[0m                 self.estimator, X, y[:, i], sample_weight)\n\u001b[1;32m--> 169\u001b[1;33m             for i in range(y.shape[1]))\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\AnacondaFolder\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\AnacondaFolder\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\AnacondaFolder\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\AnacondaFolder\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\AnacondaFolder\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\AnacondaFolder\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\AnacondaFolder\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\AnacondaFolder\\lib\\site-packages\\sklearn\\multioutput.py\u001b[0m in \u001b[0;36m_fit_estimator\u001b[1;34m(estimator, X, y, sample_weight)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\AnacondaFolder\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    328\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[1;32m--> 330\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\AnacondaFolder\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\AnacondaFolder\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\AnacondaFolder\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\AnacondaFolder\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\AnacondaFolder\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\AnacondaFolder\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\AnacondaFolder\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\AnacondaFolder\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'balanced'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\AnacondaFolder\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    814\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 816\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    817\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\AnacondaFolder\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    378\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#I used tree as a standin for the df_test result, this can be changed later\n",
    "df_tree = run_cluster_alg(df_train, df_test)\n",
    "df_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 68.4%\n"
     ]
    }
   ],
   "source": [
    "accuracy_assess(df_tree, col_name='prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shows which predictions are given too much (>1) and which too little (<1)\n",
    "predicted_correct_events = Counter(list(df_tree[df_tree['next_event'] == df_tree['prediction']]['prediction']))\n",
    "predicted_events = Counter(list(df_tree['prediction']))\n",
    "true_events = Counter(list(df_tree['next_event']))\n",
    "\n",
    "for i in predicted_events:\n",
    "    if i != i:\n",
    "        break\n",
    "    print(i)\n",
    "    per = round(predicted_events[i]/true_events[i]*100, 1)\n",
    "    print(str(per) + '%')\n",
    "    print('importance: ' + str(abs(predicted_events[i] - true_events[i])))\n",
    "    if per > 100:\n",
    "        print('given ' + str(round(per-100, 1)) + '% too much')\n",
    "    \n",
    "    elif per < 100:\n",
    "        print('given ' + str(round(100-per, 1)) + '% too little')\n",
    "    \n",
    "    else:\n",
    "        print('given exactly the right amount')\n",
    "   \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "step_nr_start, step_nr_end = 5, 500\n",
    "df_to_check = df_tree[(df_tree['step_number'] >= step_nr_start) & (df_tree['step_number'] <= step_nr_end)]\n",
    "df_confusion = pd.crosstab(df_to_check['next_event'], df_to_check['prediction'], rownames=['next_event'], colnames=['prediction'])\n",
    "df_conf_norm = df_confusion / df_confusion.sum(axis=1)\n",
    "def plot_confusion_matrix(df_confusion, title='Confusion matrix', cmap=plt.cm.gray_r):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    plt.matshow(df_confusion, cmap=cmap, fignum=1)\n",
    "    dd = list(df_confusion.columns)\n",
    "    plt.xticks([])\n",
    "    plt.yticks(range(0,len(dd)), dd)\n",
    "    plt.yticks(fontsize=7)\n",
    "    #plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.savefig('confusion_matrix.png',  bbox_inches='tight')\n",
    "    tick_marks = np.arange(len(df_confusion.columns))\n",
    "    #plt.tight_layout()\n",
    "    #plt.ylabel(df_confusion.index.name)\n",
    "    #plt.xlabel(df_confusion.columns.name)\n",
    "\n",
    "plot_confusion_matrix(df_conf_norm)\n",
    "df_conf_norm.to_csv('normalized.csv')\n",
    "df_confusion.to_csv('confusion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shows time related attributes of tree (assuming that time is stored in 'time:between_predicted')\n",
    "#create column with absolute difference between predicted time and actual time\n",
    "def time_difference_if_correct(var):\n",
    "    if var[0]:\n",
    "        return abs((var[1] - var[2]))\n",
    "    else:\n",
    "        return (0)\n",
    "    \n",
    "def apply_function(var):\n",
    "    if var[2] == var[3]:\n",
    "        return (var[0] == var[1])\n",
    "        \n",
    "    else:\n",
    "        return (var[0] == 'editor:close_case')\n",
    "        \n",
    "df_tree['shifted_case_id'] = df_tree['case_id'].shift(-1)\n",
    "df_tree.loc[len(df_tree) - 1, 'shifted_case_id'] = 0\n",
    "df_tree['correct'] = df_tree[['prediction', 'next_event', 'case_id', 'shifted_case_id']].apply(apply_function, axis=1)\n",
    "df_tree = df_tree.drop(['shifted_case_id'], axis=1)\n",
    "\n",
    "df_tree['time:relative_prediction_off'] = df_tree['time:time_between'] - df_tree['time:between_predicted']\n",
    "df_tree['time:absolute_prediction_off'] = df_tree['time:relative_prediction_off'].abs()\n",
    "\n",
    "df_tree['time:relative_prediction_correct'] = df_tree['time:time_between'] - df_tree['time:between_predicted']\n",
    "df_tree['time:absolute_prediction_correct'] = df_tree['time:relative_prediction_correct'].abs()    \n",
    "        \n",
    "for element in [(True, 'mean'), (True, 'median'), (False, 'mean'), (False, 'median')]:\n",
    "    timely_statistics(df_tree, relative=element[0], statistic=element[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov chain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create1Chain(df):\n",
    "    df_chain = df.copy()\n",
    "    chain1 = {}\n",
    "    shape = df_chain.shape[0]\n",
    "    conceptNames = list(df_chain['concept:name'])\n",
    "\n",
    "    for i, key in enumerate(conceptNames):\n",
    "        \n",
    "        if (shape > i + 1):\n",
    "            event = conceptNames[i+1]\n",
    "            \n",
    "            if (event == 'A_SUBMITTED'):\n",
    "                event = 'editor: close_case'\n",
    "                \n",
    "            if (key not in chain1):\n",
    "                chain1[key] = [event]\n",
    "            else:\n",
    "                chain1[key].append(event)\n",
    "                \n",
    "    print('Chain size: {0} distinct events.'.format(len(chain1)))\n",
    "    \n",
    "    return chain1\n",
    "\n",
    "def create2Chain(df):\n",
    "    df_chain = df.copy()\n",
    "    chain2 = {}\n",
    "    shape = df_chain.shape[0]\n",
    "    conceptNames = ['editor: first event'] + list(df_chain['concept:name'])\n",
    "    ids = ['editor: first id'] + list(df_chain['case_id']) \n",
    "\n",
    "\n",
    "    for i, key1 in enumerate(conceptNames):\n",
    "\n",
    "        if shape > i + 2:\n",
    "            key2 = conceptNames[i+1]\n",
    "            event = conceptNames[i+2]\n",
    "\n",
    "\n",
    "            if (ids[i] == ids[i+2]): \n",
    "\n",
    "                if ((key1, key2) not in chain2):\n",
    "                    chain2[(key1, key2)] = [event] \n",
    "                else:\n",
    "                    chain2[(key1, key2)].append(event)\n",
    "\n",
    "            elif (event == 'A_SUBMITTED'): \n",
    "\n",
    "                if ((key1, key2) not in chain2):\n",
    "                    chain2[(key1, key2)] = ['editor: close_case']\n",
    "                else:\n",
    "                    chain2[(key1, key2)].append('editor: close_case')\n",
    "\n",
    "            elif (key2 == 'A_SUBMITTED'):\n",
    "\n",
    "                if ((key1, key2) not in chain2):\n",
    "                    chain2[(key1, key2)] = ['A_PARTLYSUBMITTED']\n",
    "                else:\n",
    "                    chain2[(key1, key2)].append('A_PARTLYSUBMITTED')\n",
    "    \n",
    "    print('Chain size: {0} distinct event pairs.'.format(len(chain2)))\n",
    "    \n",
    "    return chain2\n",
    "\n",
    "\n",
    "def create3Chain(df, chain2):\n",
    "    df_chain = df.copy()\n",
    "    chain3 = {}\n",
    "    shape = df_chain.shape[0]\n",
    "    conceptNames = ['editor: first event'] + ['editor: second event'] + list(df_chain['concept:name'])\n",
    "    ids = ['editor: first id'] + list(df_chain['case_id'])\n",
    "    \n",
    "    for i, key1 in enumerate(conceptNames):\n",
    "        \n",
    "        if (shape > i + 3):\n",
    "            key2 = conceptNames[i+1]\n",
    "            key3 = conceptNames[i+2]\n",
    "            event = conceptNames[i+3]\n",
    "                                \n",
    "            if (key2 == 'A_SUBMITTED'):\n",
    "                event = random.choice(chain2[(key2, key3)])\n",
    "                    \n",
    "            elif (key3 == 'A_SUBMITTED'):\n",
    "                event = 'A_PARTLYSUBMITTED'\n",
    "                \n",
    "            elif (event == 'A_SUBMITTED'):\n",
    "                event = 'editor: close_case'\n",
    "                \n",
    "            if ((key1, key2, key3) not in chain3):\n",
    "                chain3[(key1, key2, key3)] = [event] \n",
    "            else:\n",
    "                chain3[(key1, key2, key3)].append(event)\n",
    "                    \n",
    "    print('Chain size: {0} distinct event trios.'.format(len(chain3)))\n",
    "    \n",
    "    return chain3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredictions(df, chain1, chain2, chain3):\n",
    "    df_2markov = df.copy()\n",
    "    nextEvents = []\n",
    "\n",
    "    for i in tqdm(range(-2, df_2markov.shape[0] - 2)):\n",
    "\n",
    "        if (i == -2):\n",
    "            event1 = 'editor: first event'\n",
    "            event2 = 'editor: second event'\n",
    "        elif (i == -1):\n",
    "            event1 = 'editor: second event'\n",
    "            event2 = df_2markov['concept:name'][i+1]\n",
    "        else:\n",
    "            event1 = df_2markov['concept:name'][i]\n",
    "            event2 = df_2markov['concept:name'][i+1]\n",
    "\n",
    "        event3 = df_2markov['concept:name'][i+2]\n",
    "        \n",
    "        if ((event1, event2, event3) in chain3):\n",
    "            nextEvents.append(random.choice(chain3[(event1, event2, event3)]))\n",
    "        else:\n",
    "            if ((event2, event3) in chain2):\n",
    "                nextEvents.append(random.choice(chain2[(event2, event3)]))\n",
    "            else:\n",
    "                nextEvents.append(random.choice(chain1[event3]))\n",
    "        \n",
    "    return nextEvents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain size: 24 distinct events.\n",
      "Chain size: 131 distinct event pairs.\n",
      "Chain size: 328 distinct event trios.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47823/47823 [00:02<00:00, 19975.05it/s]\n"
     ]
    }
   ],
   "source": [
    "df_2train = df_train.copy()\n",
    "df_2validation = df_validation.copy()\n",
    "df_2test = df_test.copy()\n",
    "\n",
    "chain1 = create1Chain(df_2train)\n",
    "chain2 = create2Chain(df_2train)\n",
    "chain3 = create3Chain(df_2train, chain2)\n",
    "\n",
    "eventPredictions = getPredictions(df_2test, chain1, chain2, chain3)\n",
    "\n",
    "df_2test['Prediction'] = eventPredictions\n",
    "\n",
    "#Prepare the next event column\n",
    "df_2test['next_event'] = df_2test['concept:name']\n",
    "df_2test.loc[df_2test['step_number'] == 0, 'next_event'] = 'editor: close_case'\n",
    "df_2test['next_event'] = df_2test['next_event'].shift(-1)\n",
    "df_2test.loc[len(df_2test) - 1, 'next_event'] = 'editor: close_case'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 75.9%\n"
     ]
    }
   ],
   "source": [
    "#The Markov chain prediction gives ~76.3% accuracy on the test set (non-deterministic)\n",
    "accuracy_assess(df_2test, col_name = 'Prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chain Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = df['concept:name'].unique().tolist() + ['editor: close_case']\n",
    "\n",
    "def createTransitionMatrix(df, chain):\n",
    "    shape = len(names)\n",
    "    transitionMatrix = np.zeros((shape, shape))\n",
    "\n",
    "    for name in names:\n",
    "        \n",
    "        if(name == 'editor: close_case'):\n",
    "            nextSteps = {}\n",
    "        else:\n",
    "            nextSteps = Counter(chain[name])\n",
    "            \n",
    "        sumOfSteps = sum(nextSteps.values())\n",
    "\n",
    "        for step in nextSteps:\n",
    "            x = names.index(name)\n",
    "            y = names.index(step)\n",
    "\n",
    "            probability = nextSteps[step] / sumOfSteps\n",
    "\n",
    "            transitionMatrix[x, y] = round(probability, 2)\n",
    "            \n",
    "    return transitionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "transitionMatrix = createTransitionMatrix(df_2validation, chain1)\n",
    "\n",
    "for i, name1 in enumerate(names):\n",
    "    G.add_node(name1, size = 25)\n",
    "    G.nodes[name1]['group'] = 1\n",
    "    \n",
    "    if (name1 == 'editor: close_case'):\n",
    "        G.nodes[name1]['group'] = 2\n",
    "    \n",
    "    for j, name2 in enumerate(names):\n",
    "        rate = transitionMatrix[i][j]\n",
    "        \n",
    "        if (rate > 0): #increase this number to reduce the complexity of graph\n",
    "            G.add_edge(name1, name2, weight = rate, label=\"{:.02f}\".format(rate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = Network('600px', '1000px', directed = True, notebook = False)\n",
    "#make notebook=True if don't want to see the graph on a seperate tab\n",
    "nt.from_nx(G)\n",
    "\n",
    "#nt.show_buttons() #to see all settings\n",
    "#to increase the distance between the nodes increase the spring length\n",
    "nt.show_buttons(filter_ = ['physics']) #to see only the physics settings\n",
    "\n",
    "nt.show('nx.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_markov_alg(df_train : pd.DataFrame, df_validation : pd.DataFrame):    \n",
    "    tracemalloc.start()\n",
    "\n",
    "    df_2validation = df_validation.copy()\n",
    "\n",
    "    chain1, chain2 = create1Chain(df_train), create2Chain(df_train)\n",
    "    chain3 = create3Chain(df_train, chain2)\n",
    "\n",
    "    eventPredictions = getPredictions(df_2validation, chain1, chain2, chain3)\n",
    "\n",
    "    df_2validation['Prediction'] = eventPredictions\n",
    "    \n",
    "    df_2validation = apply_triagram(df_2validation)\n",
    "    df_2validation.loc[df_2validation['combined P'].isnull(),'combined P'] = df_2validation['Prediction']\n",
    "    df_2validation['Prediction'] = df_2validation['combined P']\n",
    "    df_2validation = df_2validation.drop(['combined P', 'lifecycle:transition P', 'concept:name P'], axis=1)\n",
    "\n",
    "    #Prepare the next event column\n",
    "    df_2validation['next_event'] = df_2validation['concept:name']\n",
    "    df_2validation.loc[df_2validation['step_number'] == 0, 'next_event'] = 'editor: close_case'\n",
    "    df_2validation['next_event'] = df_2validation['next_event'].shift(-1)\n",
    "    df_2validation.loc[len(df) - 1, 'next_event'] = 'editor: close_case'\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    print(f\"Current memory usage is {current / 10**6}MB; Peak was {peak / 10**6}MB\")\n",
    "    tracemalloc.stop()\n",
    "    cpu = psutil.cpu_percent()\n",
    "    print('Current CPU usage is {} %'.format(psutil.cpu_percent()))\n",
    "\n",
    "    return (df_2validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain size: 24 distinct events.\n",
      "Chain size: 131 distinct event pairs.\n",
      "Chain size: 328 distinct event trios.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34767/34767 [00:02<00:00, 13183.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory usage is 8.020548MB; Peak was 24.380044MB\n",
      "Current CPU usage is 0.0 %\n"
     ]
    }
   ],
   "source": [
    "df_2validation = run_markov_alg(df_train, df_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 76.7%\n"
     ]
    }
   ],
   "source": [
    "#Checking the accuracy of this algorithm on the validation dataset (?)\n",
    "accuracy_assess(df_2validation, col_name = 'Prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runtime_calculation(df : pd.DataFrame, is_baseline : bool, is_clusterTree : bool, is_markov : bool, \n",
    "                        lengths : list, df_train=[], df_validation=[], df_test=[]) -> list:\n",
    "    '''Splits the input dataframe into a couple smaller ones, nr of rows indicated by limits (list)\n",
    "    this in case if is_baseline is True\n",
    "    In case of baseline, we are dealing with the single-dataset approach\n",
    "    Markov is vulnerable for getting previously-unseen values; you should not plug really small dataframes in it. \n",
    "    Markov is also a non-deterministic model, so some times it will stumble upon such value, some not.\n",
    "    In order to get the runtimes, you might need to run this cell call a handful of times.'''\n",
    "    \n",
    "    runtimes = [0] * len(lengths)\n",
    "    \n",
    "    if(is_baseline + is_clusterTree + is_markov > 1):\n",
    "        return(\"Please only indicate one prediction model at a time\")\n",
    "    \n",
    "    if (is_baseline): #BASELINE\n",
    "        df = preprocessing(df)\n",
    "        for i in range(0, len(lengths)):\n",
    "            print(i, lengths[i])\n",
    "            df = df[:lengths[i]]\n",
    "            time_start = time.time()\n",
    "            run_baseline(df) #The version on a single dataset:\n",
    "            time_end = time.time()\n",
    "            runtimes[i] = time_end-time_start\n",
    "            \n",
    "    elif (is_clusterTree): #CLUSTER-TREE\n",
    "        for i in range(0, len(lengths)): \n",
    "            df = df[:lengths[i]]\n",
    "            print('enountered_split')\n",
    "            df_train, df_validation, df_test = data_split(df, 0.8, 0.2)\n",
    "            time_start = time.time()\n",
    "            run_cluster_alg(df_train, df_test)\n",
    "            time_end = time.time()\n",
    "            runtimes[i] = time_end-time_start\n",
    "            \n",
    "    elif (is_markov): #MARKOV\n",
    "        for i in range(0, len(lengths)):\n",
    "            df = df[:lengths[i]]\n",
    "            print('encountered_split')\n",
    "            print(df.shape[0])\n",
    "            df_train, df_validation, df_test = data_split(df, 0.8, 0.2)\n",
    "            time_start = time.time()\n",
    "            run_markov_alg(df_train, df_validation)\n",
    "            time_end = time.time()\n",
    "            runtimes[i] = time_end-time_start\n",
    "    \n",
    "    return(runtimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 125000\n",
      "Using the entire dataset. Full dataset is returned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125000/125000 [00:28<00:00, 4359.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory usage is 119.382253MB; Peak was 134.353135MB\n",
      "Current CPU usage is 0.0 %\n",
      "1 100000\n",
      "Using the entire dataset. Full dataset is returned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:22<00:00, 4411.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory usage is 26.691679MB; Peak was 32.402632MB\n",
      "Current CPU usage is 0.0 %\n",
      "2 75000\n",
      "Using the entire dataset. Full dataset is returned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75000/75000 [00:16<00:00, 4609.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory usage is 20.074184MB; Peak was 24.361801MB\n",
      "Current CPU usage is 0.0 %\n",
      "3 50000\n",
      "Using the entire dataset. Full dataset is returned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:10<00:00, 4979.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory usage is 13.453769MB; Peak was 16.315542MB\n",
      "Current CPU usage is 0.0 %\n",
      "4 35000\n",
      "Using the entire dataset. Full dataset is returned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35000/35000 [00:09<00:00, 3752.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory usage is 9.508364MB; Peak was 11.515985MB\n",
      "Current CPU usage is 0.0 %\n",
      "5 20000\n",
      "Using the entire dataset. Full dataset is returned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:05<00:00, 3353.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory usage is 5.497149MB; Peak was 6.64977MB\n",
      "Current CPU usage is 0.0 %\n",
      "6 15000\n",
      "Using the entire dataset. Full dataset is returned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:06<00:00, 2404.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory usage is 4.024835MB; Peak was 4.890444MB\n",
      "Current CPU usage is 0.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[29.587692260742188,\n",
       " 23.529345989227295,\n",
       " 17.178982734680176,\n",
       " 10.518601894378662,\n",
       " 9.627550840377808,\n",
       " 6.263358116149902,\n",
       " 6.698383092880249]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Always order the limits descending!\n",
    "#Also, Run this on a non-sample dataset only!\n",
    "limits = [125000, 100000, 75000, 50000, 35000, 20000, 15000] #length=7\n",
    "\n",
    "runtimes = runtime_calculation(df=df, is_baseline=True, is_clusterTree=False, is_markov=False, lengths=limits)\n",
    "runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes = [125000, 100000, 75000, 50000, 35000, 20000, 15000]\n",
    "\n",
    "markov_runtimes = [2.0361163616, 1.37507867813, 1.0630609989, 0.52603006362, 0.362020730972, 0.221012830734, 0.175009965896]\n",
    "\n",
    "baseline_runtimes = [29.5876922607, 23.5293459892, 17.1789827346, 10.5186018943, 9.6275508403, 6.2633581161, 6.6983830928]\n",
    "\n",
    "clusterTree_runtimes = None #Runs too long on my PC to even try finding those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Markov_runtimes</th>\n",
       "      <th>Baseline_runtimes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>input_sizes</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.175010</td>\n",
       "      <td>6.698383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.221013</td>\n",
       "      <td>6.263358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.362021</td>\n",
       "      <td>9.627551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.526030</td>\n",
       "      <td>10.518602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>1.063061</td>\n",
       "      <td>17.178983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>1.375079</td>\n",
       "      <td>23.529346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125000</td>\n",
       "      <td>2.036116</td>\n",
       "      <td>29.587692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Markov_runtimes  Baseline_runtimes\n",
       "input_sizes                                    \n",
       "15000               0.175010           6.698383\n",
       "20000               0.221013           6.263358\n",
       "35000               0.362021           9.627551\n",
       "50000               0.526030          10.518602\n",
       "75000               1.063061          17.178983\n",
       "100000              1.375079          23.529346\n",
       "125000              2.036116          29.587692"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [125000, 100000, 75000, 50000, 35000, 20000, 15000]\n",
    "\n",
    "df = pd.DataFrame({'input_sizes': x, 'Markov_runtimes': markov_runtimes, 'Baseline_runtimes': baseline_runtimes})\n",
    "\n",
    "df = df.iloc[::-1]\n",
    "df = df.set_index('input_sizes')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = df['Baseline_runtimes']\n",
    "y = df['Markov_runtimes']\n",
    "                \n",
    "plt.plot(x)\n",
    "plt.plot(y)\n",
    "plt.title(\"Comparing the runtime of different models\")\n",
    "plt.ylabel(\"Runtime [s]\")\n",
    "plt.xlabel(\"Input size\")\n",
    "plt.legend(['Baseline', 'Markov'])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A_SUBMITTED</th>\n",
       "      <th>A_PARTLYSUBMITTED</th>\n",
       "      <th>A_PREACCEPTED</th>\n",
       "      <th>W_Completeren aanvraag</th>\n",
       "      <th>A_ACCEPTED</th>\n",
       "      <th>O_SELECTED</th>\n",
       "      <th>A_FINALIZED</th>\n",
       "      <th>O_CREATED</th>\n",
       "      <th>O_SENT</th>\n",
       "      <th>W_Nabellen offertes</th>\n",
       "      <th>...</th>\n",
       "      <th>O_ACCEPTED</th>\n",
       "      <th>A_ACTIVATED</th>\n",
       "      <th>O_CANCELLED</th>\n",
       "      <th>W_Wijzigen contractgegevens</th>\n",
       "      <th>A_DECLINED</th>\n",
       "      <th>A_CANCELLED</th>\n",
       "      <th>W_Afhandelen leads</th>\n",
       "      <th>O_DECLINED</th>\n",
       "      <th>W_Nabellen incomplete dossiers</th>\n",
       "      <th>W_Beoordelen fraude</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>case_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         A_SUBMITTED  A_PARTLYSUBMITTED  A_PREACCEPTED  \\\n",
       "case_id                                                  \n",
       "0                  1                  1              1   \n",
       "1                  1                  1              1   \n",
       "2                  1                  1              1   \n",
       "3                  1                  1              0   \n",
       "4                  1                  1              0   \n",
       "\n",
       "         W_Completeren aanvraag  A_ACCEPTED  O_SELECTED  A_FINALIZED  \\\n",
       "case_id                                                                \n",
       "0                             3           1           1            1   \n",
       "1                             5           1           2            1   \n",
       "2                             9           1           3            1   \n",
       "3                             0           0           0            0   \n",
       "4                             0           0           0            0   \n",
       "\n",
       "         O_CREATED  O_SENT  W_Nabellen offertes  ...  O_ACCEPTED  A_ACTIVATED  \\\n",
       "case_id                                          ...                            \n",
       "0                1       1                    7  ...           1            1   \n",
       "1                2       2                   10  ...           1            1   \n",
       "2                3       3                   25  ...           1            1   \n",
       "3                0       0                    0  ...           0            0   \n",
       "4                0       0                    0  ...           0            0   \n",
       "\n",
       "         O_CANCELLED  W_Wijzigen contractgegevens  A_DECLINED  A_CANCELLED  \\\n",
       "case_id                                                                      \n",
       "0                  0                            0           0            0   \n",
       "1                  1                            0           0            0   \n",
       "2                  2                            1           0            0   \n",
       "3                  0                            0           1            0   \n",
       "4                  0                            0           1            0   \n",
       "\n",
       "         W_Afhandelen leads  O_DECLINED  W_Nabellen incomplete dossiers  \\\n",
       "case_id                                                                   \n",
       "0                         0           0                               0   \n",
       "1                         0           0                               0   \n",
       "2                         0           0                               0   \n",
       "3                         0           0                               0   \n",
       "4                         0           0                               0   \n",
       "\n",
       "         W_Beoordelen fraude  \n",
       "case_id                       \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          0  \n",
       "4                          0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Group pre-cluster df by case (case_id)\n",
    "precluster_df = prepare_column_for_clustering(df['case_id'], df['concept:name'], df['concept:name'].unique())\n",
    "precluster_df_by_case = precluster_df.groupby('case_id').sum()\n",
    "precluster_df_by_case.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Perform PCA (2 component) on the data\n",
    "pca2d = PCA(n_components = 2)\n",
    "pca2d.fit(precluster_df_by_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Perform PCA (3 component) on the data\n",
    "pca3d = PCA(n_components = 3)\n",
    "pca3d.fit(precluster_df_by_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>case_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.776662</td>\n",
       "      <td>-1.766118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.483221</td>\n",
       "      <td>-3.996394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>13.335660</td>\n",
       "      <td>-14.378924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-5.451243</td>\n",
       "      <td>3.194374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-5.451243</td>\n",
       "      <td>3.194374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0          1\n",
       "case_id                      \n",
       "0         0.776662  -1.766118\n",
       "1         4.483221  -3.996394\n",
       "2        13.335660 -14.378924\n",
       "3        -5.451243   3.194374\n",
       "4        -5.451243   3.194374"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create 2d dataframe\n",
    "pca2d_df = pd.DataFrame(pca2d.transform(precluster_df_by_case))\n",
    "pca2d_df.index.names = ['case_id']\n",
    "pca2d_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>case_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.776662</td>\n",
       "      <td>-1.766118</td>\n",
       "      <td>-3.315719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.483221</td>\n",
       "      <td>-3.996394</td>\n",
       "      <td>-3.825478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>13.335660</td>\n",
       "      <td>-14.378924</td>\n",
       "      <td>-10.449408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-5.451243</td>\n",
       "      <td>3.194374</td>\n",
       "      <td>-1.030985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-5.451243</td>\n",
       "      <td>3.194374</td>\n",
       "      <td>-1.030985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0          1          2\n",
       "case_id                                 \n",
       "0         0.776662  -1.766118  -3.315719\n",
       "1         4.483221  -3.996394  -3.825478\n",
       "2        13.335660 -14.378924 -10.449408\n",
       "3        -5.451243   3.194374  -1.030985\n",
       "4        -5.451243   3.194374  -1.030985"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create 3d dataframe\n",
    "pca3d_df = pd.DataFrame(pca3d.transform(precluster_df_by_case))\n",
    "pca3d_df.index.names = ['case_id']\n",
    "pca3d_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NR_OF_CLUSTERS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-273a2e1265c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Retrieve Clusters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'concept:name cluster'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcluster_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_clusters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecluster_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-43-6a9718543e3f>\u001b[0m in \u001b[0;36mget_clusters\u001b[1;34m(prepared_rows)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;34m''''''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mkmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNR_OF_CLUSTERS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0munique_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepared_rows\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'NR_OF_CLUSTERS' is not defined"
     ]
    }
   ],
   "source": [
    "#Retrieve Clusters\n",
    "df['concept:name cluster'], cluster_model = get_clusters(precluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group regular df by case (case_id)\n",
    "df_by_case = df.groupby('case_id').mean()[['concept:name cluster']]\n",
    "df_by_case.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the dataframes for 2d\n",
    "vis2d_df = pd.merge(df_by_case, pca2d_df, on='case_id', how='left')\n",
    "vis2d_df.columns = ['cluster', 'x_comp', 'y_comp']\n",
    "vis2d_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the dataframes for 3d\n",
    "vis3d_df = pd.merge(df_by_case, pca3d_df, on='case_id', how='left')\n",
    "vis3d_df.columns = ['cluster', 'x_comp', 'y_comp', 'z_comp']\n",
    "vis3d_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot with low alpha\n",
    "sns.lmplot('x_comp', 'y_comp', data=vis2d_df, hue='cluster', fit_reg=False, height=8, aspect=2, scatter_kws={'alpha': 0.25});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot with high alpha\n",
    "sns.lmplot('x_comp', 'y_comp', data=vis2d_df, hue='cluster', fit_reg=False, height=8, aspect=2, scatter_kws={'alpha': 0.9});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_colorscale(n):\n",
    "    lst = []\n",
    "    scale = 0\n",
    "    initial = ['red','green','blue','orange','yellow','purple','black','pink','cyan','magenta']\n",
    "    for i in range(n):\n",
    "        if i < len(initial):\n",
    "            color = initial[i]\n",
    "        else:\n",
    "            color = \"#\" + \"%06x\" % randint(0, 0xFFFFFF)\n",
    "        lst.append((round(scale, 2), color))\n",
    "        scale += 1/(n-1)\n",
    "        \n",
    "    \n",
    "    return lst\n",
    "#[(0, \"red\"), (0.14, \"yellow\"), (0.29, \"purple\"), (0.43, \"green\"), (0.57, \"blue\"), (0.71, \"black\"), (0.86, \"orange\"), (1, \"pink\")]\n",
    "random_colorscale(4)\n",
    "random_colorscale(NR_OF_CLUSTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [(0, \"red\"), (0.14, \"yellow\"), (0.29, \"purple\"), (0.43, \"green\"), (0.57, \"blue\"), (0.71, \"black\"), (0.86, \"orange\"), (1, \"pink\")]\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=vis3d_df['x_comp'],\n",
    "    y=vis3d_df['y_comp'],\n",
    "    z=vis3d_df['z_comp'],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=4,\n",
    "        color=vis3d_df['cluster'],\n",
    "        colorscale=random_colorscale(14),\n",
    "        opacity=0.5\n",
    "    )\n",
    ")])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the data is already converted, I markdowned the cell below. \n",
    "\n",
    "### This is how did we convert the .xes into .csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XES TO CSV\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "\n",
    "xesToCsv_BPI = xes_importer.apply(\"Datasets/BPI_Challenge_2012.xes\")\n",
    "\n",
    "xesToCsv_listo = []\n",
    "xesToCsv_dic = {'no': -1}\n",
    "xesToCsv_prev_attr = 'no'\n",
    "xesToCsv_counter = 0\n",
    "xesToCsv_cur_attr = 'no'\n",
    "\n",
    "for i in tqdm(range(0, len(xesToCsv_BPI))):\n",
    "\n",
    "    for j in range(0, len(xesToCsv_BPI[i])):\n",
    "        xesToCsv_attr_list = list(xesToCsv_BPI[i][j])\n",
    "    \n",
    "        for k in range(0, len(xesToCsv_attr_list)):\n",
    "            xesToCsv_prev_attr = xesToCsv_cur_attr\n",
    "            xesToCsv_cur_attr = xesToCsv_attr_list[k]\n",
    "\n",
    "            if xesToCsv_cur_attr not in xesToCsv_listo:\n",
    "                xesToCsv_value = xesToCsv_dic[xesToCsv_prev_attr] + 1\n",
    "            \n",
    "                for key in xesToCsv_dic:\n",
    "                    if xesToCsv_dic[key] >= xesToCsv_value:\n",
    "                        xesToCsv_dic[key] += 1\n",
    "            \n",
    "                xesToCsv_dic[xesToCsv_cur_attr] = xesToCsv_value\n",
    "                xesToCsv_listo.insert(xesToCsv_value, xesToCsv_cur_attr)\n",
    "        \n",
    "        xesToCsv_cur_attr = 'no'\n",
    "        \n",
    "        \n",
    "xesToCsv_chain = []\n",
    "xesToCsv_event = []\n",
    "\n",
    "for i in tqdm(range(0, len(xesToCsv_BPI))):\n",
    "    for j in range(0, len(xesToCsv_BPI[i])):\n",
    "        xesToCsv_chain.append(i)\n",
    "        xesToCsv_event.append(j)\n",
    "        \n",
    "xesToCsv_df_BPI= pd.DataFrame(index=[np.array(xesToCsv_chain), np.array(xesToCsv_event)], columns = xesToCsv_listo)\n",
    "\n",
    "for i in tqdm(range(0, len(xesToCsv_BPI))):\n",
    "    \n",
    "    for j in range(0, len(xesToCsv_BPI[i])):\n",
    "\n",
    "        xesToCsv_attr = xesToCsv_BPI[i][j]\n",
    "        \n",
    "        for a in xesToCsv_attr:\n",
    "            xesToCsv_df_BPI.loc[(i, j), a] = xesToCsv_attr[a]\n",
    "            \n",
    "            \n",
    "xesToCsv_df_BPI.to_csv('Datasets/BPI_2012.csv')\n",
    "\n",
    "xesToCsv_attr_listo = []\n",
    "xesToCsv_attr_dic = {'no': -1}\n",
    "xesToCsv_prev_attr = 'no'\n",
    "xesToCsv_counter = 0\n",
    "xesToCsv_cur_attr = 'no'\n",
    "\n",
    "for i in tqdm(range(0, len(xesToCsv_BPI))):\n",
    "    xesToCsv_attr_list = list(xesToCsv_BPI[i].attributes)\n",
    "    \n",
    "    for k in range(0, len(xesToCsv_attr_list)):\n",
    "        xesToCsv_prev_attr = xesToCsv_cur_attr\n",
    "        xesToCsv_cur_attr = xesToCsv_attr_list[k]\n",
    "\n",
    "        if xesToCsv_cur_attr not in xesToCsv_attr_listo:\n",
    "            xesToCsv_value = xesToCsv_attr_dic[xesToCsv_prev_attr] + 1\n",
    "            \n",
    "            for key in xesToCsv_attr_dic:\n",
    "                if xesToCsv_attr_dic[key] >= xesToCsv_value:\n",
    "                    xesToCsv_attr_dic[key] += 1\n",
    "            \n",
    "            xesToCsv_attr_dic[xesToCsv_cur_attr] = xesToCsv_value\n",
    "            xesToCsv_attr_listo.insert(xesToCsv_value, xesToCsv_cur_attr)\n",
    "    \n",
    "    xesToCsv_cur_attr = 'no'\n",
    "    \n",
    "    \n",
    "xesToCsv_attr_chain = []\n",
    "\n",
    "for i in tqdm(range(0, len(xesToCsv_BPI))):\n",
    "    xesToCsv_attr_chain.append(i)\n",
    "    \n",
    "\n",
    "xesToCsv_df_BPI_attr = pd.DataFrame(index = [np.array(xesToCsv_attr_chain)], columns = xesToCsv_attr_listo)\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, len(xesToCsv_BPI))):\n",
    "\n",
    "    xesToCsv_attr = xesToCsv_BPI[i].attributes\n",
    "    \n",
    "    for a in xesToCsv_attr:\n",
    "        xesToCsv_df_BPI_attr.loc[i, a] = xesToCsv_attr[a]\n",
    "        \n",
    "xesToCsv_df_BPI_attr.to_csv('Datasets/BPI_attr_2012.csv')\n",
    "\n",
    "if 'Unnamed: 0' in xesToCsv_df_BPI.columns:\n",
    "    xesToCsv_df_BPI = xesToCsv_df_BPI.rename(columns={'Unnamed: 0': 'case_id', 'Unnamed: 1': 'step_number'})\n",
    "    xesToCsv_df_BPI_attr = xesToCsv_df_BPI_attr.rename(columns={'Unnamed: 0': 'case_id'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of the baseline preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def changing_columns_names(df : pd.DataFrame):\n",
    "    '''so far only works with the 2012 dataframe'''\n",
    "    if(len(df.columns) == 8):\n",
    "        df.columns = ['case_id', 'step_number', 'org:resource', 'lifecycle:transition',\n",
    "                      'concept:name', 'time:timestamp', 'time:weekday', 'time:hour']\n",
    "    \n",
    "    elif (len(df.columns) == 11):\n",
    "        df.columns = ['case_id', 'step_number', 'org:resource', 'lifecycle:transition',\n",
    "                      'concept:name', 'time:timestamp', 'time:weekday', 'time:hour', 'time:time_between',\n",
    "                     'time:day', 'time:event_count', 'time:busy_day']\n",
    "    else:\n",
    "        df.columns = ['case_id', 'step_number', 'org:resource', 'lifecycle:transition',\n",
    "                      'concept:name', 'time:timestamp', 'time:weekday', 'time:hour', 'time:time_between']\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of the baseline I think we are not using (and will not be) anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def cycles_shortcut(actions : list, concept_name : str, max_length : int, printing = False) -> list or bool:\n",
    "    '''For saving the operating time, we will try to terminate the baseline early if we get into a loop\n",
    "    max_length is the longest_trace parameter'''\n",
    "    \n",
    "    if(concept_name in actions): #the action has already been done\n",
    "        \n",
    "        if(actions[-1] == concept_name): #and it's the most recent action (self-loop)\n",
    "            \n",
    "            while(len(actions) < max_length): #filling the rest of the list with the current action if we're in a self-loop\n",
    "                actions.append(concept_name)\n",
    "        \n",
    "        else: #it is not the most recent action\n",
    "            placement = actions.index(concept_name) #locating the index of the \"duplicate\"\n",
    "            aid_array = actions[placement:] #copying the values\n",
    "            if (pritning):\n",
    "                pass\n",
    "                print(\"aid_array = \", aid_array)\n",
    "            \n",
    "            actions = actions + [0] * (max_length-len(actions)) #making [x, y, z, x] into [x, y, z, x, 0, 0, 0, ...]\n",
    "            if (printing):\n",
    "                pass\n",
    "                print(\"actions = \", actions)\n",
    "            \n",
    "            for i in range(placement+1, max_length): #iterating only over all the indices of 0's in actions\n",
    "                actions[i] = aid_array[(i-placement)%len(aid_array)] #copying the list's values over and over again\n",
    "        \n",
    "        return(actions) #This return has to be then the return of the iterated_expected_actions\n",
    "    \n",
    "    else:\n",
    "        return(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some really weird function that nobody knows what is it doin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testingBaseline_df_train = pd.read_csv('BPI_2012_train')\n",
    "testingBaseline_df_train['time:timestamp'] = testingBaseline_df_train['time:timestamp'].apply(fix_time)\n",
    "testingBaseline_df_train['time:time_between'] = testingBaseline_df_train['time:timestamp'].diff()\n",
    "testingBaseline_df_train.loc[testingBaseline_df['step_number'] == 0, 'time:time_between'] = pd.Timedelta(0)\n",
    "\n",
    "testingBaseline_df_test = pd.read_csv('BPI_2012_test')\n",
    "testingBaseline_df_test['time:timestamp'] = testingBaseline_df_test['time:timestamp'].apply(fix_time)\n",
    "testingBaseline_df_test['time:time_between'] = testingBaseline_df_test['time:timestamp'].diff()\n",
    "testingBaseline_df_test.loc[testingBaseline_df['step_number'] == 0, 'time:time_between'] = pd.Timedelta(0)\n",
    "\n",
    "#creates column with predicted data\n",
    "def prediction(prev):\n",
    "    return baseline_all_expected_events[0][prev]\n",
    "\n",
    "testingBaseline_df_test['predicted'] = testingBaseline_df_test['concept:name'].apply(prediction)\n",
    "\n",
    "#function that creates the 'correct' using a dataframe, the predcting column and the true column, it shows wether the prediction was correct\n",
    "testingBaseline_df_test['shifted_actual'] = testingBaseline_df_test['concept:name'].shift(-1)\n",
    "testingBaseline_df_test['shifted_case_id'] = testingBaseline_df_test['case_id'].shift(-1)\n",
    "    \n",
    "def apply_function(var):\n",
    "    if var[2] == var[3]:\n",
    "        return (var[0] == var[1])\n",
    "        \n",
    "    else:\n",
    "        return (var[0] == 'editor:close_case')\n",
    "        \n",
    "testingBaseline_df_test['correct'] = testingBaseline_df_test[['predicted', 'shifted_actual', 'case_id', 'shifted_case_id']].apply(apply_function, axis=1)\n",
    "testingBaseline_df_test = testingBaseline_df_test.drop(['shifted_actual', 'shifted_case_id'], axis=1)\n",
    "\n",
    "def prediction(prev):\n",
    "    return baseline_all_expected_events[1][prev]\n",
    "\n",
    "testingBaseline_df_test['time:between_predicted'] = testingBaseline_df_test['concept:name'].apply(prediction)\n",
    "\n",
    "#create column with absolute difference between predicted time and actual time\n",
    "def time_difference(var):\n",
    "    return abs((var[0] - var[1]).total_seconds())\n",
    "\n",
    "def time_difference_if_correct(var):\n",
    "    if var[0]:\n",
    "        return abs((var[1] - var[2]).total_seconds())\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "testingBaseline_df_test['time:absolute_prediction_off'] = testingBaseline_df_test[['time:time_between', 'time:between_predicted']].apply(time_difference, axis=1)\n",
    "testingBaseline_df_test['time:absolute_prediction_correct'] = testingBaseline_df_test[['correct', 'time:time_between', 'time:between_predicted']].apply(time_difference_if_correct, axis=1)\n",
    "\n",
    "#create column with relative difference between predicted time and actual time\n",
    "def time_difference(var):\n",
    "    return (var[0] - var[1]).total_seconds()\n",
    "\n",
    "def time_difference_if_correct(var):\n",
    "    if var[0]:\n",
    "        return (var[1] - var[2]).total_seconds()\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "testingBaseline_df_test['time:relative_prediction_off'] = testingBaseline_df_test[['time:time_between', 'time:between_predicted']].apply(time_difference, axis=1)\n",
    "testingBaseline_df_test['time:relative_prediction_correct'] = testingBaseline_df_test[['correct', 'time:time_between', 'time:between_predicted']].apply(time_difference_if_correct, axis=1)\n",
    "\n",
    "\n",
    "#Changed the code below to this to clean up the notebook. For refernce/troubleshooting, below code is markdowned, not erased\n",
    "for element in [(False, 'mean'), (False, 'median'), (True, 'mean'), (True, 'median')]:\n",
    "    timely_statistics(testingBaseline_df_test, relative=element[0], statistic=element[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous operating time plot was made this way:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def plot_results(limits : list, runtimes_baseline : list, runtimes_clusterTree : list, runtimes_markov : list) -> None:\n",
    "    '''Just plots the runtimes'''\n",
    "    \n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,6))\n",
    "    \n",
    "    ax1.scatter(x=limits, y=runtimes, color='#420CDA')\n",
    "    ax1.set_xlabel('Input size [n]', fontsize=13)\n",
    "    ax1.set_ylabel('Operating time [s]', fontsize=13)\n",
    "    \n",
    "    ax2.scatter(x=limits, y=runtimes, color='#AB3334')\n",
    "    ax2.set_xlabel('Input size [log(n)]', fontsize=13)\n",
    "    ax2.set_ylabel('Operating time [log(s)]', fontsize=13)\n",
    "    ax2.set_xscale(\"log\")\n",
    "    ax2.set_yscale(\"log\")\n",
    "    \n",
    "    if(is_baseline):\n",
    "        ax1.set_title('Runtime of the baseline algorithm', fontsize=16)\n",
    "        ax2.set_title(\"Runtime of the baseline algorithm (log scale)\", fontsize=16)\n",
    "    else:\n",
    "        ax1.set_title(\"Runtime of the cluster-tree algorithm\", fontsize=16)\n",
    "        ax2.set_title(\"Runtime of the cluster-tree algorithm (log scale)\", fontsize=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
