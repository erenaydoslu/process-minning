{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import datetime\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XES TO CSV\n",
    "xesToCsv_BPI = xes_importer.apply(\"Datasets/BPI_Challenge_2012.xes\")\n",
    "\n",
    "xesToCsv_listo = []\n",
    "xesToCsv_dic = {'no': -1}\n",
    "xesToCsv_prev_attr = 'no'\n",
    "xesToCsv_counter = 0\n",
    "xesToCsv_cur_attr = 'no'\n",
    "\n",
    "for i in range(0, len(xesToCsv_BPI)):\n",
    "\n",
    "    for j in range(0, len(xesToCsv_BPI[i])):\n",
    "        xesToCsv_attr_list = list(xesToCsv_BPI[i][j])\n",
    "    \n",
    "        for k in range(0, len(xesToCsv_attr_list)):\n",
    "            xesToCsv_prev_attr = xesToCsv_cur_attr\n",
    "            xesToCsv_cur_attr = xesToCsv_attr_list[k]\n",
    "\n",
    "            if xesToCsv_cur_attr not in xesToCsv_listo:\n",
    "                xesToCsv_value = xesToCsv_dic[xesToCsv_prev_attr] + 1\n",
    "            \n",
    "                for key in xesToCsv_dic:\n",
    "                    if xesToCsv_dic[key] >= xesToCsv_value:\n",
    "                        xesToCsv_dic[key] += 1\n",
    "            \n",
    "                xesToCsv_dic[xesToCsv_cur_attr] = xesToCsv_value\n",
    "                xesToCsv_listo.insert(xesToCsv_value, xesToCsv_cur_attr)\n",
    "        \n",
    "        xesToCsv_cur_attr = 'no'\n",
    "        \n",
    "        \n",
    "xesToCsv_chain = []\n",
    "xesToCsv_event = []\n",
    "\n",
    "for i in range(0, len(xesToCsv_BPI)):\n",
    "    for j in range(0, len(xesToCsv_BPI[i])):\n",
    "        xesToCsv_chain.append(i)\n",
    "        xesToCsv_event.append(j)\n",
    "        \n",
    "xesToCsv_df_BPI= pd.DataFrame(index=[np.array(xesToCsv_chain), np.array(xesToCsv_event)], columns = xesToCsv_listo)\n",
    "\n",
    "for i in range(0, len(xesToCsv_BPI)):\n",
    "    \n",
    "    for j in range(0, len(xesToCsv_BPI[i])):\n",
    "\n",
    "        xesToCsv_attr = xesToCsv_BPI[i][j]\n",
    "        \n",
    "        for a in xesToCsv_attr:\n",
    "            xesToCsv_df_BPI.loc[(i, j), a] = xesToCsv_attr[a]\n",
    "            \n",
    "            \n",
    "xesToCsv_df_BPI.to_csv('Datasets/BPI.csv')\n",
    "xesToCsv_df_BPI.to_hdf('Datasets/BPI.h5', 'data', mode='w')\n",
    "\n",
    "xesToCsv_attr_listo = []\n",
    "xesToCsv_attr_dic = {'no': -1}\n",
    "xesToCsv_prev_attr = 'no'\n",
    "xesToCsv_counter = 0\n",
    "xesToCsv_cur_attr = 'no'\n",
    "\n",
    "for i in range(0, len(xesToCsv_BPI)):\n",
    "    xesToCsv_attr_list = list(xesToCsv_BPI[i].attributes)\n",
    "    \n",
    "    for k in range(0, len(xesToCsv_attr_list)):\n",
    "        xesToCsv_prev_attr = xesToCsv_cur_attr\n",
    "        xesToCsv_cur_attr = xesToCsv_attr_list[k]\n",
    "\n",
    "        if xesToCsv_cur_attr not in xesToCsv_attr_listo:\n",
    "            xesToCsv_value = xesToCsv_attr_dic[xesToCsv_prev_attr] + 1\n",
    "            \n",
    "            for key in xesToCsv_attr_dic:\n",
    "                if xesToCsv_attr_dic[key] >= xesToCsv_value:\n",
    "                    xesToCsv_attr_dic[key] += 1\n",
    "            \n",
    "            xesToCsv_attr_dic[xesToCsv_cur_attr] = xesToCsv_value\n",
    "            xesToCsv_attr_listo.insert(xesToCsv_value, xesToCsv_cur_attr)\n",
    "    \n",
    "    xesToCsv_cur_attr = 'no'\n",
    "    \n",
    "    \n",
    "xesToCsv_attr_chain = []\n",
    "\n",
    "for i in range(0, len(xesToCsv_BPI)):\n",
    "    xesToCsv_attr_chain.append(i)\n",
    "    \n",
    "\n",
    "xesToCsv_df_BPI_attr = pd.DataFrame(index = [np.array(xesToCsv_attr_chain)], columns = xesToCsv_attr_listo)\n",
    "\n",
    "\n",
    "for i in range(0, len(xesToCsv_BPI)):\n",
    "\n",
    "    xesToCsv_attr = xesToCsv_BPI[i].attributes\n",
    "    \n",
    "    for a in xesToCsv_attr:\n",
    "        xesToCsv_df_BPI_attr.loc[i, a] = xesToCsv_attr[a]\n",
    "        \n",
    "xesToCsv_df_BPI_attr.to_csv('Datasets/BPI_attr.csv')\n",
    "xesToCsv_df_BPI_attr.to_hdf('Datasets/BPI.h5', 'data')\n",
    "\n",
    "xesToCsv_df_BPI = pd.read_hdf('Datasets/BPI.h5', 'data')\n",
    "xesToCsv_df_BPI_attr = pd.read_hdf('Datasets/BPI_attr.h5', 'data')\n",
    "xesToCsv_df_BPI = xesToCsv_df_BPI.rename(columns={'Unnamed: 0': 'chain', 'Unnamed: 1': 'event'})\n",
    "xesToCsv_df_BPI = xesToCsv_df_BPI.set_index(['chain', 'event'])\n",
    "xesToCsv_df_BPI_attr = xesToCsv_df_BPI_attr.rename(columns={'Unnamed: 0': 'chain'})\n",
    "xesToCsv_df_BPI_attr = xesToCsv_df_BPI_attr.set_index('chain')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Datasets/BPI_2012.csv')\n",
    "df_attr = pd.read_csv('Datasets/BPI_attr_2012.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Load\n",
    "\n",
    "def fix_time(time):\n",
    "    return datetime.datetime.fromisoformat(time)\n",
    "\n",
    "def load_data(BPI = 'BPI.h5', BPI_attr = 'BPI_attr.h5',  data2012 = False):\n",
    "    df_BPI = pd.read_hdf(BPI, 'data')\n",
    "    df_BPI_attr = pd.read_hdf(BPI_attr, 'data')\n",
    "    df_BPI = df_BPI.rename(columns={'Unnamed: 0': 'chain', 'Unnamed: 1': 'event'})\n",
    "    df_BPI = df_BPI.set_index(['chain', 'event'])\n",
    "    df_BPI_attr = df_BPI_attr.rename(columns={'Unnamed: 0': 'chain'})\n",
    "    df_BPI_attr = df_BPI_attr.set_index('chain')\n",
    "    \n",
    "    df_BPI['time:timestamp'] = df_BPI['time:timestamp'].apply(fix_time)\n",
    "\n",
    "    if data2012:\n",
    "        df_BPI_attr['REG_DATE'] = df_BPI_attr['REG_DATE'].apply(fix_time)\n",
    "\n",
    "def load_data_xes(data):\n",
    "    BPI = xes_importer.apply(data)\n",
    "    \n",
    "load_data(BPI = 'Datasets/BPI_2012.h5', BPI_attr = 'Datsets/BPI_attr_2012.h5', data2012 = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Data\n",
    "\n",
    "splitData_df = df\n",
    "splitData_df = splitData_df.rename(columns={'Unnamed: 0': 'case_id', 'Unnamed: 1': 'step_number'})\n",
    "splitData_df['time:timestamp'] = splitData_df['time:timestamp'].apply(fix_time)\n",
    "    \n",
    "splitData_df_attr = df_attr\n",
    "splitData_df_attr = splitData_df_attr.rename(columns={'Unnamed: 0': 'case_id'})\n",
    "splitData_df_attr['REG_DATE'] = splitData_df_attr['REG_DATE'].apply(fix_time)\n",
    "\n",
    "splitData_limit_date = datetime.datetime(2012, 2, 3, 1, 1, 1, 633000, tzinfo=datetime.timezone(datetime.timedelta(seconds=7200)))\n",
    "\n",
    "#training data_set\n",
    "splitData_df_train = splitData_df\n",
    "#df_train = pd.DataFrame(columns = (df.columns))\n",
    "\n",
    "splitData_listo = []\n",
    "\n",
    "for i in tqdm(range(0, 13087)):\n",
    "    if splitData_df[splitData_df['case_id'] == i].iloc[-1]['time:timestamp'] > splitData_limit_date:\n",
    "        splitData_listo.append(i)\n",
    "\n",
    "for i in tqdm(range(0, len(splitData_listo))):\n",
    "    splitData_df_train = splitData_df_train.drop(splitData_df_train[splitData_df_train['case_id'] == splitData_listo[i]].index)\n",
    "\n",
    "#for i in tqdm(range(0, len(listo))):\n",
    "    #df_train = df_train.append(df[df['case_id'] == listo[i]])\n",
    "\n",
    "splitData_df_train = splitData_df_train.reset_index(drop = True)\n",
    "\n",
    "#testing data_set\n",
    "splitData_df_test = pd.DataFrame(columns = (splitData_df.columns))\n",
    "splitData_listo = []\n",
    "\n",
    "for i in tqdm(range(0, 13087)):\n",
    "    if splitData_df[splitData_df['case_id'] == i].iloc[0]['time:timestamp'] > splitData_limit_date:\n",
    "        splitData_listo.append(i)\n",
    "\n",
    "for i in tqdm(range(0, len(splitData_listo))):\n",
    "    splitData_df_test = splitData_df_test.append(splitData_df[splitData_df['case_id'] == splitData_listo[i]])\n",
    "\n",
    "splitData_df_test = splitData_df_test.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline\n",
    "\n",
    "def load_dataset_csv(file_name : str, file_attribute_name : str) -> list:\n",
    "    #Loading the two datasets. So far we'll only be using the BPI.csv one\n",
    "    #This also parses the date column\n",
    "    df = pd.read_csv(file_name)\n",
    "    df['time:timestamp'] = df['time:timestamp'].apply(fix_time)\n",
    "    df_attr = pd.read_csv(file_attribute_name)\n",
    "    return df, df_attr\n",
    "\n",
    "# The current version of the tool works with the 2012 dataset!!!\n",
    "baseline_df, baseline_df_attr = load_dataset_csv('Datasets/BPI_2012.csv', 'Datasets/BPI_attr_2012.csv')\n",
    "\n",
    "#Let us change the column names to the aforementioned\n",
    "baseline_df.columns = ['case_id', 'step_number', 'org:resource', 'lifecycle:transition',\n",
    "       'concept:name', 'time:timestamp']\n",
    "\n",
    "#Erasing all the non-complete actions from the database:\n",
    "baseline_df = baseline_df[baseline_df['lifecycle:transition'] == 'COMPLETE']\n",
    "\n",
    "baseline_df = baseline_df.reset_index()\n",
    "\n",
    "#I used to suppress the output of the cell, but didn't know it also does not save any progress within this cell\n",
    "#So in order for df to update globally, we can't suppress the output unfortunately\n",
    "baseline_df.drop('index', axis=1)\n",
    "\n",
    "def compute_time_difference():\n",
    "    #Set the time difference column\n",
    "    #This function is places here because of the erased non-complete actions\n",
    "    baseline_df['time:time_between'] = baseline_df['time:timestamp'].diff()\n",
    "    baseline_df.loc[baseline_df['step_number'] == 0, 'time:time_between'] = pd.Timedelta(0)\n",
    "    \n",
    "compute_time_difference()\n",
    "\n",
    "def parse_timestamp_data():\n",
    "    # Adds extra time related columns to the dataset to be used later\n",
    "    # 0:Monday,..., 6:Sunday\n",
    "    baseline_df['time:weekday'] = [x.weekday() for x in baseline_df['time:timestamp']]\n",
    "    baseline_df['time:hour'] = [x.hour for x in baseline_df['time:timestamp']]\n",
    "    \n",
    "parse_timestamp_data()\n",
    "\n",
    "def creating_dict_for_next_step_stats (df : pd.DataFrame, concept_name : str) -> dict:\n",
    "    '''For an input action checks for all the possible next actions and counts their occurence'''\n",
    "    \n",
    "    dic_occurrence = {}\n",
    "    dic_total_time = {}\n",
    "    ids = list(df['case_id']) + ['editor: last id'] #Otherwise we check i+1-th position that does not exist\n",
    "    times = list(df['time:time_between']) + [pd.Timedelta(0)] #Otherwise we check i+1-th position that does not exist\n",
    "    names = df['concept:name']\n",
    "    df_concept = df[names == concept_name]\n",
    "    \n",
    "    for i, row in df_concept.iterrows():\n",
    "        \n",
    "        if (ids[i] == ids[i+1]): #an instance of the same case\n",
    "            \n",
    "            if (names[i+1] not in dic_occurrence):\n",
    "                dic_occurrence[names[i+1]] = 1\n",
    "                dic_total_time[names[i+1]] = times[i+1]\n",
    "            else:\n",
    "                dic_occurrence[names[i+1]] += 1\n",
    "                dic_total_time[names[i+1]] += times[i+1]\n",
    "                \n",
    "        else: #the last instance of the case\n",
    "            \n",
    "            if ('editor: close_case' not in dic_occurrence):\n",
    "                dic_occurrence['editor: close_case'] = 1\n",
    "                dic_total_time['editor: close_case'] = times[i+1]\n",
    "            else:\n",
    "                dic_occurrence['editor: close_case'] += 1\n",
    "                dic_total_time['editor: close_case'] += times[i+1]\n",
    "    \n",
    "    #Compute average time\n",
    "    dic_avg_time = {}\n",
    "    \n",
    "    for key in dic_total_time:\n",
    "        dic_avg_time[key] = dic_total_time[key] / dic_occurrence[key]\n",
    "        \n",
    "    return(dic_occurrence, dic_avg_time)\n",
    "\n",
    "creating_dict_for_next_step_stats(baseline_df, 'A_SUBMITTED')\n",
    "\n",
    "def choosing_next_action(dic : dict):\n",
    "    '''Finds the max value of the input dict and returns the key of the max value'''\n",
    "    \n",
    "    max_key = max(dic, key=dic.get)\n",
    "    \n",
    "    return(max_key)\n",
    "\n",
    "choosing_next_action({'A_PARTLYSUBMITTED': 910, \"wow\": 21, \"not_wow\": 37})\n",
    "\n",
    "def cycles_shortcut(actions : list, concept_name : str, max_length : int) -> list or bool:\n",
    "    '''For saving the operating time, we will try to terminate the baseline early if we get into a loop\n",
    "    max_length is the longest_trace parameter'''\n",
    "    \n",
    "    if(concept_name in actions): #the action has already been done\n",
    "        \n",
    "        if(actions[-1] == concept_name): #and it's the most recent action (self-loop)\n",
    "            \n",
    "            while(len(actions) < max_length): #filling the rest of the list with the current action if we're in a self-loop\n",
    "                actions.append(concept_name)\n",
    "        \n",
    "        else: #it is not the most recent action\n",
    "            placement = actions.index(concept_name) #locating the index of the \"duplicate\"\n",
    "            aid_array = actions[placement:] #copying the values\n",
    "            print(\"aid_array = \", aid_array)\n",
    "            \n",
    "            actions = actions + [0] * (max_length-len(actions)) #making [x, y, z, x] into [x, y, z, x, 0, 0, 0, ...]\n",
    "            print(\"actions = \", actions)\n",
    "            \n",
    "            for i in range(placement+1, max_length): #iterating only over all the indices of 0's in actions\n",
    "                actions[i] = aid_array[(i-placement)%len(aid_array)] #copying the list's values over and over again\n",
    "        \n",
    "        return(actions) #This return has to be then the return of the iterated_expected_actions\n",
    "    \n",
    "    else:\n",
    "        return(False)\n",
    "    \n",
    "cycles_shortcut([2, 1, 3, 7], 3, 15)\n",
    "\n",
    "def iterating_expected_actions(df : pd.DataFrame, concept_name : str, n : int) -> list:\n",
    "    '''concept_name is the starting point (first action)\n",
    "    n is the length of the longest trace ever observed\n",
    "    It is stored in lonest_trace but for runtime reasons, use n so far'''\n",
    "    \n",
    "    longest_trace = max(df['step_number']) #finding the longest trace in the database (nr of steps)\n",
    "    #note that we determine this AFTER deleting some rows with uncomplete steps. We should be running this on full df\n",
    "    \n",
    "    i = 0\n",
    "    actions = [concept_name] #Here is the list that will store all the subsequent actions the algorithm decices to perform\n",
    "    while (i < n): #terminate if we are exceeding the max number of steps\n",
    "        wow = creating_dict_for_next_step_stats(df, concept_name)[0] #list all possible options\n",
    "        concept_name = choosing_next_action(wow) #Choose the most commonly used option\n",
    "        \n",
    "        if(cycles_shortcut(actions, concept_name, n) != False): #Checks if we are stuck in a loop\n",
    "            print(\"we are stuck in a loop\")\n",
    "            return(cycles_shortcut(actions, concept_name, n))\n",
    "        \n",
    "        if(concept_name == 'editor: close_case'): #If it is the \"terminate\" option - terminate\n",
    "            break\n",
    "        actions.append(concept_name) #Add the action to the list\n",
    "        i += 1\n",
    "    \n",
    "    actions.append('editor: close_case')\n",
    "    print('i = ', i, \"n = \", n)\n",
    "    \n",
    "    return(actions)\n",
    "\n",
    "def add_expected_events(df : pd.DataFrame) -> list:\n",
    "    all_events = df['concept:name'].unique()\n",
    "    next_event_name_dic = {'editor: close_case': 'editor: close_case'}\n",
    "    next_event_duration_dic = {'editor: close_case': pd.Timedelta(0)}\n",
    "    \n",
    "    for event in all_events:\n",
    "        next_step_stats = creating_dict_for_next_step_stats(df, event)\n",
    "        wow = next_step_stats[0] #list all possible options\n",
    "        concept_name = choosing_next_action(wow) #Choose the most commonly used option\n",
    "        next_event_name_dic[event] = concept_name\n",
    "        next_event_duration_dic[event] = next_step_stats[1][concept_name]\n",
    "    \n",
    "    return next_event_name_dic, next_event_duration_dic\n",
    "\n",
    "#Get list of all expected next events and the expected time till that next event\n",
    "baseline_all_expected_events = add_expected_events(baseline_df)\n",
    "baseline_all_expected_events\n",
    "\n",
    "#Add column to dataframe with expected next events and times\n",
    "baseline_df['expect:next_event'] = baseline_df['concept:name'].map(baseline_all_expected_events[0])\n",
    "baseline_df['expect:next_time'] = baseline_df['concept:name'].map(baseline_all_expected_events[1]) + baseline_df['time:timestamp']\n",
    "\n",
    "baseline_df.to_csv('BPI_with_predictions.csv')\n",
    "\n",
    "iterating_expected_actions(baseline_df, 'A_SUBMITTED', 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline Performance Testing\n",
    "\n",
    "baseline_limits = [50, 100, 200, 500, 1000, 2000, 5000, 10000, 20000, 50000, 100000]\n",
    "baseline_runtimes = [0] * len(baseline_limits)\n",
    "\n",
    "for i in range(0, len(baseline_limits)):\n",
    "    print(i)\n",
    "    df_small = baseline_df[:baseline_limits[i]]\n",
    "    time_start = time.time()\n",
    "    iterating_expected_actions(df_small, 'A_SUBMITTED', 15)\n",
    "    time_end = time.time()\n",
    "    baseline_runtimes[i] = time_end-time_start\n",
    "\n",
    "baseline_runtimes\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "baseline_wow = plt.figure(figsize=(7.5, 5))\n",
    "plt.scatter(x = baseline_limits, y=baseline_runtimes, color = '#AB3334')\n",
    "plt.title('Runtime of the baseline algorithm (log scale)', fontsize = 16)\n",
    "plt.ylabel('Operating time [log(s)]', fontsize = 13)\n",
    "plt.xlabel('Input size [log(n)]', fontsize = 13)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale('log')\n",
    "\n",
    "baseline_wow2 = plt.figure(figsize=(7.5, 5))\n",
    "plt.scatter(x = baseline_limits, y=baseline_runtimes, color = '#420CDA')\n",
    "plt.title('Runtime of the baseline algorithm', fontsize = 16)\n",
    "plt.ylabel('Operating time [s]', fontsize = 13)\n",
    "plt.xlabel('Input size [n]', fontsize = 13)\n",
    "\n",
    "baseline_f, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,6))\n",
    "\n",
    "ax1.scatter(x = baseline_limits, y=baseline_runtimes, color = '#420CDA')\n",
    "ax1.set_title('Runtime of the baseline algorithm', fontsize = 16)\n",
    "ax1.set_xlabel('Input size [n]', fontsize = 13)\n",
    "ax1.set_ylabel('Operating time [s]', fontsize = 13)\n",
    "\n",
    "ax2.scatter(x = baseline_limits, y=baseline_runtimes, color = '#AB3334')\n",
    "ax2.set_title('Runtime of the baseline algorithm (log scale)', fontsize = 16)\n",
    "ax2.set_ylabel('Operating time [log(s)]', fontsize = 13)\n",
    "ax2.set_xlabel('Input size [log(n)]', fontsize = 13)\n",
    "ax2.set_xscale(\"log\")\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "baseline_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the Baseline\n",
    "\n",
    "testingBaseline_df = df\n",
    "testingBaseline_df = testingBaseline_df.rename(columns={'Unnamed: 0': 'case_id', 'Unnamed: 1': 'step_number'})\n",
    "testingBaseline_df['time:timestamp'] = testingBaseline_df['time:timestamp'].apply(fix_time)\n",
    "    \n",
    "testingBaseline_df_attr = df_attr\n",
    "testingBaseline_df_attr = testingBaseline_df_attr.rename(columns={'Unnamed: 0': 'case_id'})\n",
    "testingBaseline_df_attr['REG_DATE'] = testingBaseline_df_attr['REG_DATE'].apply(fix_time)\n",
    "\n",
    "testingBaseline_df = testingBaseline_df[testingBaseline_df['lifecycle:transition'] == 'COMPLETE']\n",
    "testingBaseline_df = testingBaseline_df.reset_index(drop = True)\n",
    "\n",
    "testingBaseline_df['time:time_between'] = testingBaseline_df['time:timestamp'].diff()\n",
    "testingBaseline_df.loc[testingBaseline_df['step_number'] == 0, 'time:time_between'] = pd.Timedelta(0)\n",
    "\n",
    "testingBaseline_df['time:weekday'] = [x.weekday() for x in testingBaseline_df['time:timestamp']]\n",
    "testingBaseline_df['time:hour'] = [x.hour for x in testingBaseline_df['time:timestamp']]  \n",
    "\n",
    "#amount of total data that is training data:\n",
    "testingBaseline_train_data = 0.8\n",
    "#amount of train data that is validation data:\n",
    "testingBaseline_validation_data = 0.1\n",
    "\n",
    "testingBaseline_limit_date = testingBaseline_df[testingBaseline_df['case_id'] == round(testingBaseline_df.iloc[-1, 0]*testingBaseline_train_data)].iloc[0]['time:timestamp']\n",
    "#limit_date = datetime.datetime(2012, 2, 3, 1, 1, 1, 633000, tzinfo=datetime.timezone(datetime.timedelta(seconds=7200))) \n",
    "\n",
    "#training data_set\n",
    "testingBaseline_df_train = testingBaseline_df\n",
    "testingBaseline_listo = []\n",
    "\n",
    "for i in tqdm(range(0, 13087)):\n",
    "    if testingBaseline_df[testingBaseline_df['case_id'] == i].iloc[-1]['time:timestamp'] > testingBaseline_limit_date:\n",
    "        testingBaseline_listo.append(i)\n",
    "\n",
    "for i in tqdm(range(0, len(testingBaseline_listo))):\n",
    "    testingBaseline_df_train = testingBaseline_df_train.drop(testingBaseline_df_train[testingBaseline_df_train['case_id']==testingBaseline_listo[i]].index)\n",
    "\n",
    "testingBaseline_df_train = testingBaseline_df_train.reset_index(drop = True)\n",
    "\n",
    "#testing data_set\n",
    "testingBaseline_mask = np.random.rand(len(testingBaseline_df_train)) < testingBaseline_validation_data\n",
    "testingBaseline_df_validation = testingBaseline_df_train[testingBaseline_mask]\n",
    "testingBaseline_df_train = testingBaseline_df_train[~testingBaseline_mask]\n",
    "\n",
    "#testing data_set\n",
    "testingBaseline_df_test = pd.DataFrame(columns = (testingBaseline_df.columns))\n",
    "testingBaseline_listo = []\n",
    "\n",
    "for i in tqdm(range(0, 13087)):\n",
    "    if testingBaseline_df[testingBaseline_df['case_id'] == i].iloc[0]['time:timestamp'] > testingBaseline_limit_date:\n",
    "        testingBaseline_listo.append(i)\n",
    "\n",
    "for i in tqdm(range(0, len(testingBaseline_listo))):\n",
    "    testingBaseline_df_test = testingBaseline_df_test.append(testingBaseline_df[testingBaseline_df['case_id'] == testingBaseline_listo[i]])\n",
    "\n",
    "testingBaseline_df_test = testingBaseline_df_test.reset_index(drop = True)\n",
    "\n",
    "testingBaseline_df_train.to_csv('BPI_2012_train', index=False)\n",
    "\n",
    "testingBaseline_df_validation.to_csv('BPI_2012_validation')\n",
    "\n",
    "testingBaseline_df_test.to_csv('BPI_2012_test', index=False)\n",
    "\n",
    "testingBaseline_df_train = pd.read_csv('BPI_2012_train')\n",
    "testingBaseline_df_train['time:timestamp'] = testingBaseline_df_train['time:timestamp'].apply(fix_time)\n",
    "testingBaseline_df_train['time:time_between'] = testingBaseline_df_train['time:timestamp'].diff()\n",
    "testingBaseline_df_train.loc[testingBaseline_df['step_number'] == 0, 'time:time_between'] = pd.Timedelta(0)\n",
    "\n",
    "testingBaseline_df_test = pd.read_csv('BPI_2012_test')\n",
    "testingBaseline_df_test['time:timestamp'] = testingBaseline_df_test['time:timestamp'].apply(fix_time)\n",
    "testingBaseline_df_test['time:time_between'] = testingBaseline_df_test['time:timestamp'].diff()\n",
    "testingBaseline_df_test.loc[testingBaseline_df['step_number'] == 0, 'time:time_between'] = pd.Timedelta(0)\n",
    "\n",
    "def prediction(prev):\n",
    "    return baseline_all_expected_events[0][prev]\n",
    "\n",
    "testingBaseline_df_test['predicted'] = testingBaseline_df_test['concept:name'].apply(prediction)\n",
    "\n",
    "testingBaseline_df_test['correct'] = np.nan\n",
    "\n",
    "for i in tqdm(range(0, len(testingBaseline_df_test)-1)):\n",
    "    \n",
    "    if testingBaseline_df_test.loc[i, 'case_id'] == testingBaseline_df_test.loc[i+1, 'case_id']:\n",
    "        testingBaseline_df_test.loc[i, 'correct'] = (testingBaseline_df_test.loc[i, 'predicted'] == testingBaseline_df_test.loc[i+1, 'concept:name'])\n",
    "    \n",
    "    else:\n",
    "         testingBaseline_df_test.loc[i, 'correct'] = (testingBaseline_df_test.loc[i, 'predicted'] == 'editor:close_case')\n",
    "            \n",
    "list(testingBaseline_df_test['correct']).count(True)/len(list(testingBaseline_df_test['correct']))\n",
    "\n",
    "def prediction(prev):\n",
    "    return baseline_all_expected_events[1][prev]\n",
    "\n",
    "testingBaseline_df_test['time:between_predicted'] = testingBaseline_df_test['concept:name'].apply(prediction)\n",
    "\n",
    "def time_difference(var):\n",
    "    return abs((var[0] - var[1]).total_seconds())\n",
    "\n",
    "testingBaseline_df_test['time:predicton_off'] = testingBaseline_df_test[['time:time_between', 'time:between_predicted']].apply(time_difference, axis=1)\n",
    "\n",
    "seconds = testingBaseline_df_test['time:predicton_off'].mean()\n",
    "minutes = seconds/60\n",
    "hours = minutes/60\n",
    "days = hours/24\n",
    "print(str(seconds) + ', ' + str(minutes) + ', ' + str(hours) + ', ' + str(days))\n",
    "\n",
    "seconds = testingBaseline_df_test['time:predicton_off'].median()\n",
    "minutes = seconds/60\n",
    "hours = minutes/60\n",
    "days = hours/24\n",
    "print(str(seconds) + ', ' + str(minutes) + ', ' + str(hours) + ', ' + str(days))\n",
    "\n",
    "145472/60/24"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
