{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import datetime\n",
    "import random\n",
    "import time\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn import preprocessing as pre\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import math\n",
    "import tracemalloc\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the data is already converted, I markdowned the cell below. \n",
    "\n",
    "### This is how did we convert the .xes into .csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#XES TO CSV\n",
    "xesToCsv_BPI = xes_importer.apply(\"Datasets/BPI_Challenge_2012.xes\")\n",
    "\n",
    "xesToCsv_listo = []\n",
    "xesToCsv_dic = {'no': -1}\n",
    "xesToCsv_prev_attr = 'no'\n",
    "xesToCsv_counter = 0\n",
    "xesToCsv_cur_attr = 'no'\n",
    "\n",
    "for i in tqdm(range(0, len(xesToCsv_BPI))):\n",
    "\n",
    "    for j in range(0, len(xesToCsv_BPI[i])):\n",
    "        xesToCsv_attr_list = list(xesToCsv_BPI[i][j])\n",
    "    \n",
    "        for k in range(0, len(xesToCsv_attr_list)):\n",
    "            xesToCsv_prev_attr = xesToCsv_cur_attr\n",
    "            xesToCsv_cur_attr = xesToCsv_attr_list[k]\n",
    "\n",
    "            if xesToCsv_cur_attr not in xesToCsv_listo:\n",
    "                xesToCsv_value = xesToCsv_dic[xesToCsv_prev_attr] + 1\n",
    "            \n",
    "                for key in xesToCsv_dic:\n",
    "                    if xesToCsv_dic[key] >= xesToCsv_value:\n",
    "                        xesToCsv_dic[key] += 1\n",
    "            \n",
    "                xesToCsv_dic[xesToCsv_cur_attr] = xesToCsv_value\n",
    "                xesToCsv_listo.insert(xesToCsv_value, xesToCsv_cur_attr)\n",
    "        \n",
    "        xesToCsv_cur_attr = 'no'\n",
    "        \n",
    "        \n",
    "xesToCsv_chain = []\n",
    "xesToCsv_event = []\n",
    "\n",
    "for i in tqdm(range(0, len(xesToCsv_BPI))):\n",
    "    for j in range(0, len(xesToCsv_BPI[i])):\n",
    "        xesToCsv_chain.append(i)\n",
    "        xesToCsv_event.append(j)\n",
    "        \n",
    "xesToCsv_df_BPI= pd.DataFrame(index=[np.array(xesToCsv_chain), np.array(xesToCsv_event)], columns = xesToCsv_listo)\n",
    "\n",
    "for i in tqdm(range(0, len(xesToCsv_BPI))):\n",
    "    \n",
    "    for j in range(0, len(xesToCsv_BPI[i])):\n",
    "\n",
    "        xesToCsv_attr = xesToCsv_BPI[i][j]\n",
    "        \n",
    "        for a in xesToCsv_attr:\n",
    "            xesToCsv_df_BPI.loc[(i, j), a] = xesToCsv_attr[a]\n",
    "            \n",
    "            \n",
    "xesToCsv_df_BPI.to_csv('Datasets/BPI_2012.csv')\n",
    "\n",
    "xesToCsv_attr_listo = []\n",
    "xesToCsv_attr_dic = {'no': -1}\n",
    "xesToCsv_prev_attr = 'no'\n",
    "xesToCsv_counter = 0\n",
    "xesToCsv_cur_attr = 'no'\n",
    "\n",
    "for i in tqdm(range(0, len(xesToCsv_BPI))):\n",
    "    xesToCsv_attr_list = list(xesToCsv_BPI[i].attributes)\n",
    "    \n",
    "    for k in range(0, len(xesToCsv_attr_list)):\n",
    "        xesToCsv_prev_attr = xesToCsv_cur_attr\n",
    "        xesToCsv_cur_attr = xesToCsv_attr_list[k]\n",
    "\n",
    "        if xesToCsv_cur_attr not in xesToCsv_attr_listo:\n",
    "            xesToCsv_value = xesToCsv_attr_dic[xesToCsv_prev_attr] + 1\n",
    "            \n",
    "            for key in xesToCsv_attr_dic:\n",
    "                if xesToCsv_attr_dic[key] >= xesToCsv_value:\n",
    "                    xesToCsv_attr_dic[key] += 1\n",
    "            \n",
    "            xesToCsv_attr_dic[xesToCsv_cur_attr] = xesToCsv_value\n",
    "            xesToCsv_attr_listo.insert(xesToCsv_value, xesToCsv_cur_attr)\n",
    "    \n",
    "    xesToCsv_cur_attr = 'no'\n",
    "    \n",
    "    \n",
    "xesToCsv_attr_chain = []\n",
    "\n",
    "for i in tqdm(range(0, len(xesToCsv_BPI))):\n",
    "    xesToCsv_attr_chain.append(i)\n",
    "    \n",
    "\n",
    "xesToCsv_df_BPI_attr = pd.DataFrame(index = [np.array(xesToCsv_attr_chain)], columns = xesToCsv_attr_listo)\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, len(xesToCsv_BPI))):\n",
    "\n",
    "    xesToCsv_attr = xesToCsv_BPI[i].attributes\n",
    "    \n",
    "    for a in xesToCsv_attr:\n",
    "        xesToCsv_df_BPI_attr.loc[i, a] = xesToCsv_attr[a]\n",
    "        \n",
    "xesToCsv_df_BPI_attr.to_csv('Datasets/BPI_attr_2012.csv')\n",
    "\n",
    "if 'Unnamed: 0' in xesToCsv_df_BPI.columns:\n",
    "    xesToCsv_df_BPI = xesToCsv_df_BPI.rename(columns={'Unnamed: 0': 'case_id', 'Unnamed: 1': 'step_number'})\n",
    "    xesToCsv_df_BPI_attr = xesToCsv_df_BPI_attr.rename(columns={'Unnamed: 0': 'case_id'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the data with some safety-features for common mishaps we had"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_time(time):\n",
    "    return (datetime.datetime.fromisoformat(time))\n",
    "\n",
    "def load_data(BPI = 'BPI.csv', BPI_attr = 'BPI_attr.csv',  data2012 = False, sample=False):\n",
    "    df_BPI = pd.read_csv(BPI)\n",
    "    df_BPI_attr = pd.read_csv(BPI_attr)\n",
    "    \n",
    "    if 'Unnamed: 0' in df_BPI.columns:\n",
    "        df_BPI = df_BPI.rename(columns={'Unnamed: 0': 'case_id', 'Unnamed: 1': 'step_number'})\n",
    "        df_BPI_attr = df_BPI_attr.rename(columns={'Unnamed: 0': 'case_id'})\n",
    "    \n",
    "    df_BPI['time:timestamp'] = df_BPI['time:timestamp'].apply(fix_time)\n",
    "\n",
    "    df_BPI_attr['REG_DATE'] = df_BPI_attr['REG_DATE'].apply(fix_time)\n",
    "    \n",
    "    df_BPI['time:weekday'] = [x.weekday() for x in df_BPI['time:timestamp']]\n",
    "    df_BPI['time:hour'] = [x.hour for x in df_BPI['time:timestamp']]\n",
    "    \n",
    "    if(sample):\n",
    "        df_BPI, df_BPI_attr = df_BPI[:50000], df_BPI_attr[:2359]\n",
    "    \n",
    "    return (df_BPI, df_BPI_attr)\n",
    "\n",
    "def load_data_xes(data):\n",
    "    BPI = xes_importer.apply(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Warning: if sample=True, please bear it in mind. If you have any loops iterating over the entire df, don't rig them\n",
    "df, df_attr = load_data(BPI = 'Datasets/BPI_2012.csv', BPI_attr = 'Datasets/BPI_attr_2012.csv', sample=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data splitting:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bellow code splits the data and saves it. This code takes around 9 minutes to run. Thus we have added the pre-split data to the zip file, which can be loaded in the next part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(df : pd.DataFrame, amount_train_data = 0.8, valitest_size = 0.2) -> tuple:\n",
    "    '''Plug in the df and train and validation data percentages\n",
    "    \n",
    "    the amount of train data splits the data in test and train/validation data\n",
    "    The valitest_size determines how much of the train/validation set in validation'''\n",
    "    \n",
    "    amount_validation_data = 1 - amount_train_data\n",
    "    \n",
    "    #Renaming the column names if neccesary and creating a copy of the input df:\n",
    "    splitData_df = df.rename(columns={'Unnamed: 0': 'case_id', 'Unnamed: 1': 'step_number'})\n",
    "    splitData_df_attr = df_attr.rename(columns={'Unnamed: 0': 'case_id'})\n",
    "    \n",
    "    #get the limit date\n",
    "    last_case_id = list(splitData_df['case_id'])[-1]\n",
    "    splitData_limit_date = splitData_df[splitData_df['case_id'] == round(last_case_id * amount_train_data)]\n",
    "    splitData_limit_date = list(splitData_limit_date['time:timestamp'])[0]\n",
    "\n",
    "    #divide the dateset into before and after the limit date\n",
    "    before_limit_date = splitData_df[splitData_df['time:timestamp'] < splitData_limit_date]\n",
    "    after_limit_date = splitData_df[splitData_df['time:timestamp'] >= splitData_limit_date]\n",
    "    \n",
    "    #get the test datasets\n",
    "    test_data_cases = list(after_limit_date[after_limit_date['step_number'] == 0]['case_id'])\n",
    "    df_test = splitData_df[splitData_df['case_id'].isin(test_data_cases)]\n",
    "\n",
    "    #get the cases for the train and validation data set\n",
    "    final_step_before_limit_date = pd.Series(before_limit_date.step_number.values,index=before_limit_date.case_id).to_dict()\n",
    "    final_steps = pd.Series(splitData_df.step_number.values,index=splitData_df.case_id).to_dict()\n",
    "    train_data_cases = [k for k in final_step_before_limit_date if k in final_steps and final_steps[k] == final_step_before_limit_date[k]]\n",
    "\n",
    "    #randomly divide up the train/validaiton data sets\n",
    "    mask = len(train_data_cases) * valitest_size\n",
    "    random.shuffle(train_data_cases)\n",
    "    df_validation = splitData_df[splitData_df['case_id'].isin(train_data_cases[:int(mask)])]\n",
    "    df_train = splitData_df[splitData_df['case_id'].isin(train_data_cases[int(mask):])]\n",
    "    \n",
    "    #Re-setting the index:\n",
    "    df_train, df_validation, df_test = df_train.reset_index(drop=True), df_validation.reset_index(drop=True), df_test.reset_index(drop=True)\n",
    "    \n",
    "    return(df_train, df_validation, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the sample data for this split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_validation, df_test = data_split(df, 0.835, 0.525)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('Datasets/trainExample.csv')\n",
    "df_validation.to_csv('Datasets/validationExample.csv')\n",
    "df_test.to_csv('Datasets/testExample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training, validation and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_validation, df_test = pd.read_csv('Datasets/trainExample.csv'), pd.read_csv('Datasets/validationExample.csv'), pd.read_csv('Datasets/testExample.csv')\n",
    "#df_train['time:timestamp'], df_validation['time:timestamp'], df_test['time:timestamp'] = df_train['time:timestamp'].apply(fix_time), df_validation['time:timestamp'].apply(fix_time), df_test['time:timestamp'].apply(fix_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline:\n",
    "\n",
    "### Pre-processing part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changing_columns_names(df : pd.DataFrame, data2012=True, data2017=False):\n",
    "    '''so far only works with the 2012 dataframe'''\n",
    "    \n",
    "    if(len(df.columns) == 8):\n",
    "        df.columns = ['case_id', 'step_number', 'org:resource', 'lifecycle:transition',\n",
    "                      'concept:name', 'time:timestamp', 'time:weekday', 'time:hour']\n",
    "    else:\n",
    "        df.columns = ['case_id', 'step_number', 'org:resource', 'lifecycle:transition',\n",
    "                      'concept:name', 'time:timestamp', 'time:weekday', 'time:hour', 'time:time_between']\n",
    "#     elif(data2017):\n",
    "#         df.columns = ['case_id', 'step_number', 'OfferID', 'Action', 'FirstWithdrawalAmount', 'NumberOfTerms', 'Accepted', \n",
    "#                       'org:resource', 'concept:name', 'MonthlyCost', 'EventOrigin', 'EventID', 'Selected', 'CreditScore', \n",
    "#                       'lifecycle:transition', 'OfferedAmount', 'time:timestamp', 'time:weekday', 'time:hour']\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "#Erasing all the non-complete actions from the database:\n",
    "def erasing_noncomplete(df : pd.DataFrame):\n",
    "    '''Performance depends on the type of the input dataframe\n",
    "    \n",
    "    Not so wise!\n",
    "    \n",
    "    Event class: conceptname+lifecycletransition'''\n",
    "    \n",
    "    df = df[df['lifecycle:transition'] == 'COMPLETE']\n",
    "    df = df.reset_index(drop=True)\n",
    "        \n",
    "    return(df)\n",
    "\n",
    "def compute_time_difference(df : pd.DataFrame):\n",
    "    '''Set the time difference column\n",
    "    This function is places here because of the erased non-complete actions\n",
    "    \n",
    "    It has been a no-input, no-return function, so I fixed this ~ Rav'''\n",
    "\n",
    "    df['time:time_between'] = df['time:timestamp'].diff()\n",
    "    #df.loc[df['step_number'] == 0, 'time:time_between'] = pd.Timedelta(0)\n",
    "    df.loc[0, 'time:time_between'] = pd.Timedelta(0)\n",
    "    \n",
    "    #This one is required for cluster-tree but crashes the baseline, fml:\n",
    "#     df[\"time:time_between\"] = [int(x.total_seconds()) for x in df[\"time:time_between\"]]\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "def preprocessing(df):\n",
    "    '''runs the above 3 functions'''\n",
    "    \n",
    "    df = changing_columns_names(df)\n",
    "    df = erasing_noncomplete(df)\n",
    "    df = compute_time_difference(df)\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "### The algorithm's functions (only run 'baseline', it has the other ones embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_dict_for_next_step_stats (df : pd.DataFrame, concept_name : str) -> dict:\n",
    "    '''For an input action checks for all the possible next actions and counts their occurence'''\n",
    "    \n",
    "    dic_occurrence = {}\n",
    "    dic_total_time = {}\n",
    "    ids = list(df['case_id']) + ['editor: last id'] #Otherwise we check i+1-th position that does not exist\n",
    "    times = list(df['time:time_between']) + [pd.Timedelta(0)] #Otherwise we check i+1-th position that does not exist\n",
    "    names = df['concept:name']\n",
    "    df_concept = df[names == concept_name]\n",
    "    \n",
    "    for i, row in df_concept.iterrows():\n",
    "        \n",
    "        if (ids[i] == ids[i+1]): #an instance of the same case\n",
    "            \n",
    "            if (names[i+1] not in dic_occurrence):\n",
    "                dic_occurrence[names[i+1]] = 1\n",
    "                dic_total_time[names[i+1]] = times[i+1]\n",
    "            else:\n",
    "                dic_occurrence[names[i+1]] += 1\n",
    "                dic_total_time[names[i+1]] += times[i+1]\n",
    "                \n",
    "        else: #the last instance of the case\n",
    "            \n",
    "            if ('editor: close_case' not in dic_occurrence):\n",
    "                dic_occurrence['editor: close_case'] = 1\n",
    "                dic_total_time['editor: close_case'] = times[i+1]\n",
    "            else:\n",
    "                dic_occurrence['editor: close_case'] += 1\n",
    "                dic_total_time['editor: close_case'] += times[i+1]\n",
    "    \n",
    "    #Compute average time\n",
    "    dic_avg_time = {}\n",
    "    \n",
    "    for key in dic_total_time:\n",
    "        dic_avg_time[key] = dic_total_time[key] / dic_occurrence[key]\n",
    "        \n",
    "    return(dic_occurrence, dic_avg_time)\n",
    "\n",
    "def choosing_next_action(dic : dict):\n",
    "    '''Finds the max value of the input dict and returns the key of the max value'''\n",
    "    \n",
    "    max_key = max(dic, key=dic.get)\n",
    "    return(max_key)\n",
    "\n",
    "\n",
    "def cycles_shortcut(actions : list, concept_name : str, max_length : int, printing = False) -> list or bool:\n",
    "    '''For saving the operating time, we will try to terminate the baseline early if we get into a loop\n",
    "    max_length is the longest_trace parameter'''\n",
    "    \n",
    "    if(concept_name in actions): #the action has already been done\n",
    "        \n",
    "        if(actions[-1] == concept_name): #and it's the most recent action (self-loop)\n",
    "            \n",
    "            while(len(actions) < max_length): #filling the rest of the list with the current action if we're in a self-loop\n",
    "                actions.append(concept_name)\n",
    "        \n",
    "        else: #it is not the most recent action\n",
    "            placement = actions.index(concept_name) #locating the index of the \"duplicate\"\n",
    "            aid_array = actions[placement:] #copying the values\n",
    "            if (pritning):\n",
    "                pass\n",
    "                print(\"aid_array = \", aid_array)\n",
    "            \n",
    "            actions = actions + [0] * (max_length-len(actions)) #making [x, y, z, x] into [x, y, z, x, 0, 0, 0, ...]\n",
    "            if (printing):\n",
    "                pass\n",
    "                print(\"actions = \", actions)\n",
    "            \n",
    "            for i in range(placement+1, max_length): #iterating only over all the indices of 0's in actions\n",
    "                actions[i] = aid_array[(i-placement)%len(aid_array)] #copying the list's values over and over again\n",
    "        \n",
    "        return(actions) #This return has to be then the return of the iterated_expected_actions\n",
    "    \n",
    "    else:\n",
    "        return(False)\n",
    "\n",
    "def add_expected_events(df : pd.DataFrame) -> list:\n",
    "    all_events = df['concept:name'].unique()\n",
    "    next_event_name_dic = {'editor: close_case': 'editor: close_case'}\n",
    "    next_event_duration_dic = {'editor: close_case': pd.Timedelta(0)}\n",
    "    \n",
    "    for event in all_events:\n",
    "        next_step_stats = creating_dict_for_next_step_stats(df, event)\n",
    "        wow = next_step_stats[0] #list all possible options\n",
    "        concept_name = choosing_next_action(wow) #Choose the most commonly used option\n",
    "        next_event_name_dic[event] = concept_name\n",
    "        next_event_duration_dic[event] = next_step_stats[1][concept_name]\n",
    "    \n",
    "    return next_event_name_dic, next_event_duration_dic\n",
    "\n",
    "def baseline(df : pd.DataFrame, save_to_csv = False) -> pd.DataFrame:\n",
    "    '''Runs all the necessary functions to add the predictions to the input dataframe\n",
    "    \n",
    "    Where is the longest trace accounted for?'''\n",
    "    \n",
    "    #Adding next expected events:\n",
    "    all_expected_events = add_expected_events(df)\n",
    "    df['expect:next_event'] = df['concept:name'].map(all_expected_events[0])\n",
    "    \n",
    "    #Adding next expected times:\n",
    "    loop_max = df.shape[0]\n",
    "    next_time = [0] * loop_max\n",
    "    mapa = df['concept:name'].map(all_expected_events[1])\n",
    "    for i in tqdm(range(0, loop_max)):\n",
    "        next_time[i] = mapa[i] + df['time:timestamp'][i]\n",
    "    \n",
    "    df['expect:next_time'] = next_time\n",
    "    \n",
    "    #Saving the results to a csv\n",
    "    if (save_to_csv):\n",
    "        df.to_csv('BPI_with_predictions.csv')\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "### Executions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocessing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baseline():    \n",
    "    tracemalloc.start()\n",
    "    baseline_df = baseline(df, save_to_csv=False) #Will only take one short loading slider to load\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    print(f\"Current memory usage is {current / 10**6}MB; Peak was {peak / 10**6}MB\")\n",
    "    tracemalloc.stop()\n",
    "    print('Current CPU usage is {} %'.format(psutil.cpu_percent()))\n",
    "    return baseline_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_baseline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance testing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set sample datasets of the below sizes to test the runtime of any algorithm on them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runtime_calculation(df : pd.DataFrame, is_baseline : bool, lengths : list,\n",
    "                       df_train=None, df_test=None) -> list:\n",
    "    '''Splits the input dataframe into a couple smaller ones, nr of rows indicated by limits (list)\n",
    "    this in case if is_baseline is True\n",
    "    \n",
    "    If is_baseline is False, we are dealing with cluster-tree and then, pass df_train and df_test alongside df'''\n",
    "    \n",
    "    runtimes = [0] * len(lengths)\n",
    "    if (is_baseline): #BASELINE\n",
    "        for i in range(0, len(lengths)):\n",
    "            print(i, lengths[i])\n",
    "            df = df[:lengths[i]]\n",
    "            time_start = time.time()\n",
    "            baseline(df, save_to_csv=False) #TO DO: check for the longest trace in the dataset\n",
    "            time_end = time.time()\n",
    "            runtimes[i] = time_end-time_start\n",
    "    else: #CLUSTER-TREE\n",
    "        \n",
    "        if (df_train == df_test == None):\n",
    "            print('enountered_split')\n",
    "            df_train, df_validation, df_test = data_split(df, 0.835, 0.525)\n",
    "            #df_train, df_validation, df_test = preprocessing(df_train), preprocessing(df_validation), preprocessing(df_test)\n",
    "        for i in range(0, len(lengths)):\n",
    "#             ratio_train = df_train.shape[0] / (df_train.shape[0] + df_test.shape[0])\n",
    "#             rows_train = math.floor(lengths[i] * ratio_train)\n",
    "#             print(\"ratio_train =\", ratio_train, \"rows_train =\", rows_train)\n",
    "            \n",
    "#             df_train, df_test = df[:rows_train], df[rows_train:]\n",
    "            \n",
    "            time_start = time.time()\n",
    "            cluster_tree_predict(df_train, df_test)\n",
    "            time_end = time.time()\n",
    "            runtimes[i] = time_end-time_start\n",
    "    \n",
    "    return(runtimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Always order the limits descending!\n",
    "#Also, using 100k on the sample df is worthless, it essentially repeats the 50k baseline\n",
    "limits = [100000, 50000, 20000, 10000, 5000, 2000, 1000, 500, 200, 100, 50] #length=10\n",
    "limits = [2000, 1000, 500, 200, 100, 50]\n",
    "\n",
    "baseline_runtimes = runtime_calculation(df=df, is_baseline=True, lengths=limits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cell below needs fixing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limits = [1000, 500, 200, 100, 50]\n",
    "\n",
    "runtimes_cluster_tree = runtime_calculation(df=df, lengths=limits, is_baseline=False, df_train=None, df_test=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting its runtimes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(limits : list, runtimes : list, is_baseline : bool) -> None:\n",
    "    '''Just plots the runtimes'''\n",
    "    \n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,6))\n",
    "    \n",
    "    ax1.scatter(x=limits, y=runtimes, color='#420CDA')\n",
    "    ax1.set_xlabel('Input size [n]', fontsize=13)\n",
    "    ax1.set_ylabel('Operating time [s]', fontsize=13)\n",
    "    \n",
    "    ax2.scatter(x=limits, y=runtimes, color='#AB3334')\n",
    "    ax2.set_xlabel('Input size [log(n)]', fontsize=13)\n",
    "    ax2.set_ylabel('Operating time [log(s)]', fontsize=13)\n",
    "    ax2.set_xscale(\"log\")\n",
    "    ax2.set_yscale(\"log\")\n",
    "    \n",
    "    if(is_baseline):\n",
    "        ax1.set_title('Runtime of the baseline algorithm', fontsize=16)\n",
    "        ax2.set_title(\"Runtime of the baseline algorithm (log scale)\", fontsize=16)\n",
    "    else:\n",
    "        ax1.set_title(\"Runtime of the cluster-tree algorithm\", fontsize=16)\n",
    "        ax2.set_title(\"Runtime of the cluster-tree algorithm (log scale)\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(limits=limits, runtimes=baseline_runtimes, is_baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(limits=limits, runtimes=runtimes_cluster_tree, is_baseline=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing the accuracy of predictions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple function to print out some statistics of a model (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timely_statistics(df_tree : pd.DataFrame, relative : bool, statistic : str) -> None:\n",
    "    '''I automated the prints below ~ Rav\n",
    "    possibly invoke with:\n",
    "    for element in [(False, 'mean'), (False, 'median'), (True, 'mean'), (True, 'median')]:\n",
    "        timely_statistics(df, relative=element[0], statistic=element[1])'''\n",
    "    \n",
    "    if(statistic == 'mean' and relative == False):\n",
    "        seconds1, seconds2 = df_tree['time:absolute_prediction_off'].mean(), df_tree['time:absolute_prediction_correct'].mean()\n",
    "        minutes1, hours1, days1, minutes2, hours2, days2 = seconds1/60, seconds1/3600, seconds1/86400, seconds2/60, seconds2/3600, seconds2/86400\n",
    "        print('absolute off mean: ' + str(seconds1) + ', ' + str(minutes1) + ', ' + str(hours1) + ', ' + str(days1))\n",
    "        print('absolute correct mean: ' + str(seconds2) + ', ' + str(minutes2) + ', ' + str(hours2) + ', ' + str(days2))\n",
    "        \n",
    "    if(statistic == 'median' and relative == False):\n",
    "        seconds1, seconds2 = df_tree['time:absolute_prediction_off'].median(), df_tree['time:absolute_prediction_correct'].median()\n",
    "        minutes1, hours1, days1, minutes2, hours2, days2 = seconds1/60, seconds1/3600, seconds1/86400, seconds2/60, seconds2/3600, seconds2/86400\n",
    "        print('absolute off median: ' + str(seconds1) + ', ' + str(minutes1) + ', ' + str(hours1) + ', ' + str(days1))\n",
    "        print('absolute correct median: ' + str(seconds2) + ', ' + str(minutes2) + ', ' + str(hours2) + ', ' + str(days2))\n",
    "        \n",
    "    if(statistic == 'mean' and relative==True):\n",
    "        df_tree['time:absolute_prediction_off'].hist(bins=10)\n",
    "        seconds1, seconds2 = df_tree['time:relative_prediction_off'].mean(), df_tree['time:relative_prediction_correct'].mean()\n",
    "        minutes1, hours1, days1, minutes2, hours2, days2 = seconds1/60, seconds1/3600, seconds1/86400, seconds2/60, seconds2/3600, seconds2/86400\n",
    "        print('relative off mean: ' + str(seconds1) + ', ' + str(minutes1) + ', ' + str(hours1) + ', ' + str(days1))\n",
    "        print('relative correct mean: ' + str(seconds2) + ', ' + str(minutes2) + ', ' + str(hours2) + ', ' + str(days2))\n",
    "        \n",
    "    if(statistic == 'median' and relative==True):\n",
    "        df_tree['time:absolute_prediction_off'].hist(bins=10)\n",
    "        seconds1, seconds2 = df_tree['time:relative_prediction_off'].median(), df_tree['time:relative_prediction_correct'].median()\n",
    "        minutes1, hours1, days1, minutes2, hours2, days2 = seconds1/60, seconds1/3600, seconds1/86400, seconds2/60, seconds2/3600, seconds2/86400\n",
    "        print('relative off median: ' + str(seconds1) + ', ' + str(minutes1) + ', ' + str(hours1) + ', ' + str(days1))\n",
    "        print('relative correct median: ' + str(seconds2) + ', ' + str(minutes2) + ', ' + str(hours2) + ', ' + str(days2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingBaseline_df_train = pd.read_csv('BPI_2012_train')\n",
    "testingBaseline_df_train['time:timestamp'] = testingBaseline_df_train['time:timestamp'].apply(fix_time)\n",
    "testingBaseline_df_train['time:time_between'] = testingBaseline_df_train['time:timestamp'].diff()\n",
    "testingBaseline_df_train.loc[testingBaseline_df['step_number'] == 0, 'time:time_between'] = pd.Timedelta(0)\n",
    "\n",
    "testingBaseline_df_test = pd.read_csv('BPI_2012_test')\n",
    "testingBaseline_df_test['time:timestamp'] = testingBaseline_df_test['time:timestamp'].apply(fix_time)\n",
    "testingBaseline_df_test['time:time_between'] = testingBaseline_df_test['time:timestamp'].diff()\n",
    "testingBaseline_df_test.loc[testingBaseline_df['step_number'] == 0, 'time:time_between'] = pd.Timedelta(0)\n",
    "\n",
    "#creates column with predicted data\n",
    "def prediction(prev):\n",
    "    return baseline_all_expected_events[0][prev]\n",
    "\n",
    "testingBaseline_df_test['predicted'] = testingBaseline_df_test['concept:name'].apply(prediction)\n",
    "\n",
    "#function that creates the 'correct' using a dataframe, the predcting column and the true column, it shows wether the prediction was correct\n",
    "testingBaseline_df_test['shifted_actual'] = testingBaseline_df_test['concept:name'].shift(-1)\n",
    "testingBaseline_df_test['shifted_case_id'] = testingBaseline_df_test['case_id'].shift(-1)\n",
    "    \n",
    "def apply_function(var):\n",
    "    if var[2] == var[3]:\n",
    "        return (var[0] == var[1])\n",
    "        \n",
    "    else:\n",
    "        return (var[0] == 'editor:close_case')\n",
    "        \n",
    "testingBaseline_df_test['correct'] = testingBaseline_df_test[['predicted', 'shifted_actual', 'case_id', 'shifted_case_id']].apply(apply_function, axis=1)\n",
    "testingBaseline_df_test = testingBaseline_df_test.drop(['shifted_actual', 'shifted_case_id'], axis=1)\n",
    "\n",
    "def prediction(prev):\n",
    "    return baseline_all_expected_events[1][prev]\n",
    "\n",
    "testingBaseline_df_test['time:between_predicted'] = testingBaseline_df_test['concept:name'].apply(prediction)\n",
    "\n",
    "#create column with absolute difference between predicted time and actual time\n",
    "def time_difference(var):\n",
    "    return abs((var[0] - var[1]).total_seconds())\n",
    "\n",
    "def time_difference_if_correct(var):\n",
    "    if var[0]:\n",
    "        return abs((var[1] - var[2]).total_seconds())\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "testingBaseline_df_test['time:absolute_prediction_off'] = testingBaseline_df_test[['time:time_between', 'time:between_predicted']].apply(time_difference, axis=1)\n",
    "testingBaseline_df_test['time:absolute_prediction_correct'] = testingBaseline_df_test[['correct', 'time:time_between', 'time:between_predicted']].apply(time_difference_if_correct, axis=1)\n",
    "\n",
    "#create column with relative difference between predicted time and actual time\n",
    "def time_difference(var):\n",
    "    return (var[0] - var[1]).total_seconds()\n",
    "\n",
    "def time_difference_if_correct(var):\n",
    "    if var[0]:\n",
    "        return (var[1] - var[2]).total_seconds()\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "testingBaseline_df_test['time:relative_prediction_off'] = testingBaseline_df_test[['time:time_between', 'time:between_predicted']].apply(time_difference, axis=1)\n",
    "testingBaseline_df_test['time:relative_prediction_correct'] = testingBaseline_df_test[['correct', 'time:time_between', 'time:between_predicted']].apply(time_difference_if_correct, axis=1)\n",
    "\n",
    "\n",
    "#Changed the code below to this to clean up the notebook. For refernce/troubleshooting, below code is markdowned, not erased\n",
    "for element in [(False, 'mean'), (False, 'median'), (True, 'mean'), (True, 'median')]:\n",
    "    timely_statistics(testingBaseline_df_test, relative=element[0], statistic=element[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triagram method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_triagram(df : pd.DataFrame, lifecycle_name = 'lifecycle:transition P', concept_name = 'concept:name P') -> pd.DataFrame:\n",
    "    '''creates row lifecycle_name (called 'lifecycle:transition P' by default\n",
    "    and concept_name (called 'concept:name P' by default, which contains the prediction we know for certain.\n",
    "    If a value can not be predicted like this it is given the np.nan value'''\n",
    "    \n",
    "    df['concept:name S1'] = df['concept:name'].shift(1)\n",
    "    df['concept:name S2'] = df['concept:name'].shift(2)\n",
    "    df['lifecycle:transition S1'] = df['lifecycle:transition'].shift(1)\n",
    "    df['lifecycle:transition S2'] = df['lifecycle:transition'].shift(2)\n",
    "    df['case_id S1'] = df['case_id'].shift(1)\n",
    "    df['case_id S2'] = df['case_id'].shift(2)\n",
    "\n",
    "\n",
    "    first_case = df['step_number'] == 0\n",
    "    second_case = df['step_number'] == 1\n",
    "\n",
    "    triagram1 = df['lifecycle:transition S1']+df['lifecycle:transition S2'] == 'COMPLETESCHEDULE'\n",
    "    triagram2 = df['concept:name S1']+df['concept:name S1'] == 'WW'\n",
    "    triagram3 = df['concept:name S1'] != df['concept:name S1']\n",
    "    triagram4 = df['case_id'] == df['case_id S1']\n",
    "    triagram5 = df['case_id'] == df['case_id S2']\n",
    "\n",
    "    df['lifecycle:transition P'] = np.nan\n",
    "    df['concept:name P'] = np.nan\n",
    "\n",
    "    df.loc[first_case, lifecycle_name] = 'COMPLETE'\n",
    "    df.loc[second_case, lifecycle_name] = 'COMPLETE'\n",
    "    df.loc[(triagram1&triagram2&triagram3&triagram4&triagram5), 'concept:name P'] = 'START'\n",
    "\n",
    "    df.loc[first_case, concept_name] = 'A_SUBMITTED'\n",
    "    df.loc[second_case, concept_name] = 'A_PARTLYSUBMITTED'\n",
    "    df.loc[(triagram1&triagram2&triagram3&triagram4&triagram5), 'concept:name P'] = df.loc[(triagram1&triagram2&triagram3&triagram4&triagram5), 'concept:name S2']\n",
    "\n",
    "    df = df.drop(['concept:name S1', 'concept:name S2', 'lifecycle:transition S1', 'lifecycle:transition S2', 'case_id S1', 'case_id S2'], axis=1)\n",
    "    df['combined P'] = df['lifecycle:transition P'] + ' + ' + df['concept:name P']\n",
    "    df['combined P'] = df['combined P'].shift(-1)\n",
    "    df['combined P'] = df['combined P'].map({'COMPLETE + A_SUBMITTED': 'editor: close_case'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = apply_triagram(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe_for_clustering(raw_df):\n",
    "    ''''''\n",
    "    \n",
    "    df = raw_df.copy()\n",
    "    df['combined_names'] = df['lifecycle:transition'] + ' + ' + df['concept:name']\n",
    "    \n",
    "    #Prepare the time between columns\n",
    "    if (type(df['time:timestamp'].iloc[0]) != datetime.datetime):\n",
    "        df['time:timestamp'] = df['time:timestamp'].apply(fix_time)\n",
    "\n",
    "    df[\"time:time_between\"] = df[\"time:timestamp\"].diff()\n",
    "    df.loc[0, 'time:time_between'] = pd.Timedelta(0) #Changed it a bit to always insert 0 into the first row\n",
    "    df[\"time:time_between\"] = [int(x.total_seconds()) for x in df[\"time:time_between\"]]\n",
    "    \n",
    "    #Prepare the next event column\n",
    "    df[\"next_event\"] = df[\"combined_names\"]\n",
    "    df.loc[df['step_number'] == 0, 'next_event'] = 'editor: close_case'\n",
    "    df[\"next_event\"] = df[\"next_event\"].shift(-1)\n",
    "    df.loc[len(df) - 1, 'next_event'] = 'editor: close_case'\n",
    "    \n",
    "    return (df)\n",
    "\n",
    "\n",
    "def prepare_column_for_clustering(case_id_column, cluster_column, unique_values):\n",
    "    ''''''\n",
    "    \n",
    "    grouped_df = pd.DataFrame({'case_id': case_id_column, 'column': cluster_column})\n",
    "    \n",
    "    for val in unique_values:\n",
    "        grouped_df[val] = 0\n",
    "        grouped_df.loc[grouped_df['column'] == val, val] = 1\n",
    "    \n",
    "    return grouped_df[['case_id'] + list(unique_values)]\n",
    "\n",
    "\n",
    "def get_clusters(prepared_rows):\n",
    "    ''''''\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=NR_OF_CLUSTERS)\n",
    "    \n",
    "    unique_columns = list(prepared_rows.columns)\n",
    "    unique_columns.pop(0)\n",
    "\n",
    "    df_grouped = prepared_rows.groupby('case_id')[unique_columns].sum()\n",
    "    kmeans.fit(df_grouped[unique_columns])\n",
    "    prediction = kmeans.labels_\n",
    "    unique_case_ids = prepared_rows['case_id'].unique()\n",
    "    return prepared_rows.case_id.map({unique_case_ids[i]: prediction[i] for i in range(len(prediction))}), kmeans\n",
    "\n",
    "\n",
    "def get_tree(df, clusters, MAX_DEPTH):\n",
    "    ''''''\n",
    "    \n",
    "    model = MultiOutputClassifier(RandomForestClassifier(n_estimators=N_ESTIMATORS, max_depth=MAX_DEPTH))\n",
    "    \n",
    "    X_dtc = df[clusters + ['step_number', 'time:weekday', 'time:hour', 'lifecycle:transition int']]\n",
    "    y_dtc = df[['next_event int', 'time:time_between']]\n",
    "\n",
    "    model.fit(X_dtc, y_dtc)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_clusters(cluster_model, prepared_rows, unique_columns):\n",
    "    ''''''\n",
    "    \n",
    "    df_grouped = prepared_rows.groupby('case_id')[unique_columns].sum()\n",
    "    prediction = cluster_model.predict(df_grouped[unique_columns])\n",
    "    unique_case_ids = prepared_rows['case_id'].unique()\n",
    "    return prepared_rows.case_id.map({unique_case_ids[i]: prediction[i] for i in range(len(prediction))})\n",
    "\n",
    "\n",
    "def predict_value(tree, df_predict, clusters):\n",
    "    ''''''\n",
    "    \n",
    "    X_dtc = df_predict[clusters + ['step_number', 'time:weekday', 'time:hour', 'lifecycle:transition int']]\n",
    "    \n",
    "    prediction = tree.predict(X_dtc)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_tree_predict(df_train, df_test):\n",
    "    ''''''\n",
    "    \n",
    "    #I splitted this into 2 lines solely for debugging purposes:\n",
    "    cluster_tree_train = prepare_dataframe_for_clustering(df_train)\n",
    "    cluster_tree_test = prepare_dataframe_for_clustering(df_test)\n",
    "    \n",
    "    le = pre.LabelEncoder()\n",
    "    le.fit(cluster_tree_train['lifecycle:transition'].unique())\n",
    "    cluster_tree_train['lifecycle:transition int'] = le.transform(cluster_tree_train['lifecycle:transition'])\n",
    "    cluster_tree_test['lifecycle:transition int'] = le.transform(cluster_tree_test['lifecycle:transition'])\n",
    "    \n",
    "    le2 = pre.LabelEncoder()\n",
    "    unique_ones = np.concatenate((cluster_tree_train['next_event'].unique(), cluster_tree_test['next_event'].unique()))\n",
    "    le2.fit(unique_ones)\n",
    "    cluster_tree_train['next_event int'] = le2.transform(cluster_tree_train['next_event'])\n",
    "    cluster_tree_test['next_event int'] = le2.transform(cluster_tree_test['next_event'])\n",
    "    \n",
    "    #Clustering\n",
    "    cluster_columns = ['combined_names'] # ENTER CLUSTER COLUMN NAMES (max 2) HERE\n",
    "    assigned_columns = [cluster_columns[i] + ' cluster' for i in range(len(cluster_columns))]\n",
    "    \n",
    "    for i in range(len(cluster_columns)):\n",
    "        #Make sure no NaN values exist in these columns\n",
    "        cluster_tree_train[cluster_columns[i]] = cluster_tree_train[cluster_columns[i]].fillna(-1)\n",
    "        cluster_tree_test[cluster_columns[i]] = cluster_tree_test[cluster_columns[i]].fillna(-1)\n",
    "        \n",
    "        unique_values = list(cluster_tree_train[cluster_columns[i]].unique())\n",
    "        unique_values_test = cluster_tree_test[cluster_columns[i]].unique()\n",
    "        \n",
    "        unique_values.extend(x for x in unique_values_test if x not in unique_values)\n",
    "        \n",
    "        prepared_train_data = prepare_column_for_clustering(cluster_tree_train['case_id'], cluster_tree_train[cluster_columns[i]], unique_values)\n",
    "        cluster_tree_train[assigned_columns[i]], cluster_model = get_clusters(prepared_train_data)\n",
    "\n",
    "        prepared_test_data = prepare_column_for_clustering(cluster_tree_test['case_id'], cluster_tree_test[cluster_columns[i]], unique_values)\n",
    "        cluster_tree_test[assigned_columns[i]] = predict_clusters(cluster_model, prepared_test_data, unique_values)\n",
    "    \n",
    "    first_steps_train = cluster_tree_train['step_number'] <= CLUSTER_STEP_SPLIT\n",
    "    first_steps_test = cluster_tree_test['step_number'] <= CLUSTER_STEP_SPLIT\n",
    "    \n",
    "    model = get_tree(cluster_tree_train[first_steps_train], ['next_event int'], BEGIN_TREE_DEPTH)\n",
    "    result = predict_value(model, cluster_tree_test[first_steps_test], ['next_event int'])\n",
    "    cluster_tree_test.loc[first_steps_test, 'prediction'] = le2.inverse_transform(result[:, 0])\n",
    "    cluster_tree_test.loc[first_steps_test, 'time:between_predicted'] = result[:, 1]\n",
    "    \n",
    "    for i in tqdm(range(NR_OF_CLUSTERS)):\n",
    "        next_amount = NR_OF_CLUSTERS\n",
    "        if len(cluster_columns) == 1:\n",
    "            next_amount = 1\n",
    "        for j in range(next_amount):\n",
    "            first_cluster_train = (cluster_tree_train[assigned_columns[0]] == i)\n",
    "            first_cluster_test = (cluster_tree_test[assigned_columns[0]] == i) & (cluster_tree_test['step_number'] > CLUSTER_STEP_SPLIT)\n",
    "            \n",
    "            if len(cluster_columns) == 1:\n",
    "                clustered_train_df = cluster_tree_train[first_cluster_train]\n",
    "                clustered_test_df = cluster_tree_test[first_cluster_test]\n",
    "            else:\n",
    "                clustered_train_df = cluster_tree_train[first_cluster_train & (cluster_tree_test[assigned_columns[1]] == j)]\n",
    "                clustered_test_df = cluster_tree_test[first_cluster_test & (cluster_tree_test[assigned_columns[1]] == j)]\n",
    "        \n",
    "            if clustered_train_df.size > 0:\n",
    "                model = get_tree(clustered_train_df, assigned_columns, CLUSTER_TREE_DEPTH)\n",
    "        \n",
    "                if clustered_test_df.size > 0:\n",
    "                    if len(cluster_columns) == 1:\n",
    "                        result = predict_value(model, clustered_test_df, assigned_columns)\n",
    "                        cluster_tree_test.loc[first_cluster_test, 'prediction'] = le2.inverse_transform(result[:, 0])\n",
    "                        cluster_tree_test.loc[first_cluster_test, 'time:between_predicted'] = result[:, 1]\n",
    "                    else:\n",
    "                        result = predict_value(model, clustered_test_df, assigned_columns)\n",
    "                        cluster_tree_test.loc[first_cluster_test & (cluster_tree_test[assigned_columns[1]] == j), 'prediction'] = le2.inverse_transform(result[:, 0])\n",
    "                        cluster_tree_test.loc[first_cluster_test & (cluster_tree_test[assigned_columns[1]] == j), 'time:between_predicted'] = result[:, 1]\n",
    "    \n",
    "    cluster_tree_test = apply_triagram(cluster_tree_test)\n",
    "    cluster_tree_test.loc[cluster_tree_test['combined P'].isnull(),'combined P'] = cluster_tree_test['prediction']\n",
    "    cluster_tree_test['prediction'] = cluster_tree_test['combined P']\n",
    "    cluster_tree_test = cluster_tree_test.drop(['combined P', 'lifecycle:transition int', 'next_event int', 'combined_names cluster', 'lifecycle:transition P', 'concept:name P'], axis=1)\n",
    "        \n",
    "    return cluster_tree_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model testing using test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NR_OF_CLUSTERS = 14\n",
    "CLUSTER_STEP_SPLIT, BEGIN_TREE_DEPTH, CLUSTER_TREE_DEPTH, N_ESTIMATORS = 7, 7, 12, 75\n",
    "\n",
    "# 7, 7, 12, 75 = 69.4%\n",
    "# 9, 5, 12, 75 = 69.1%\n",
    "# 6, 6, 10, 75 = 68.3\n",
    "# 6, 6, 8, 100 = 67.5%\n",
    "# 5, 5, 10, 100 = 67.3%\n",
    "# 7, 7, 15, 50 = 66.2%\n",
    "# 4, 4, 10, 125 = 66.2\n",
    "# 6, 4, 10, 100 = 66.0%\n",
    "# 5, 5, 7, 125 = 65.5%\n",
    "# 5, 3, 10, 125 = 64.5%\n",
    "# 5, 3, 7, 125 = 63.6%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I used tree as a standin for the df_test result, this can be changed later\n",
    "df_tree = cluster_tree_predict(df_train, df_test)\n",
    "df_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prints accuracy of the tree dataset\n",
    "accuracy = round((len(df_tree[df_tree['next_event'] == df_tree['prediction']])/len(df_tree))*100, 1)\n",
    "print('accuracy: ' + str(accuracy) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shows which predictions are given too much (>1) and which too little (<1)\n",
    "predicted_correct_events = Counter(list(df_tree[df_tree['next_event'] == df_tree['prediction']]['prediction']))\n",
    "predicted_events = Counter(list(df_tree['prediction']))\n",
    "true_events = Counter(list(df_tree['next_event']))\n",
    "\n",
    "for i in predicted_events:\n",
    "    if i != i:\n",
    "        break\n",
    "    print(i)\n",
    "    per = round(predicted_events[i]/true_events[i]*100, 1)\n",
    "    print(str(per) + '%')\n",
    "    if per > 100:\n",
    "        print('given ' + str(round(per-100, 1)) + '% too much')\n",
    "    \n",
    "    elif per < 100:\n",
    "        print('given ' + str(round(100-per, 1)) + '% too little')\n",
    "    \n",
    "    else:\n",
    "        print('given exactly the right amount')\n",
    "   \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shows time related attributes of tree (assuming that time is stored in 'time:between_predicted')\n",
    "#create column with absolute difference between predicted time and actual time\n",
    "def time_difference_if_correct(var):\n",
    "    if var[0]:\n",
    "        return abs((var[1] - var[2]))\n",
    "    else:\n",
    "        return (0)\n",
    "    \n",
    "def apply_function(var):\n",
    "    if var[2] == var[3]:\n",
    "        return (var[0] == var[1])\n",
    "        \n",
    "    else:\n",
    "        return (var[0] == 'editor:close_case')\n",
    "        \n",
    "df_tree['shifted_case_id'] = df_tree['case_id'].shift(-1)\n",
    "df_tree.loc[len(df_tree) - 1, 'shifted_case_id'] = 0\n",
    "df_tree['correct'] = df_tree[['prediction', 'next_event', 'case_id', 'shifted_case_id']].apply(apply_function, axis=1)\n",
    "df_tree = df_tree.drop(['shifted_case_id'], axis=1)\n",
    "\n",
    "df_tree['time:relative_prediction_off'] = df_tree['time:time_between'] - df_tree['time:between_predicted']\n",
    "df_tree['time:absolute_prediction_off'] = df_tree['time:relative_prediction_off'].abs()\n",
    "\n",
    "df_tree['time:relative_prediction_correct'] = df_tree['time:time_between'] - df_tree['time:between_predicted']\n",
    "df_tree['time:absolute_prediction_correct'] = df_tree['time:relative_prediction_correct'].abs()    \n",
    "        \n",
    "for element in [(True, 'mean'), (True, 'median'), (False, 'mean'), (False, 'median')]:\n",
    "    timely_statistics(df_tree, relative=element[0], statistic=element[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_markov = df.copy()\n",
    "\n",
    "df_markov = compute_time_difference(df_markov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = df_markov['concept:name'].unique().tolist() + ['editor: close_case']\n",
    "shape = len(names)\n",
    "\n",
    "transitionMatrix = np.zeros((shape, shape))\n",
    "\n",
    "def getTransitionMatrix():\n",
    "\n",
    "    for name in names:\n",
    "        nextSteps = creating_dict_for_next_step_stats(df_markov, name)[0]\n",
    "        sumOfSteps = sum(nextSteps.values())\n",
    "\n",
    "        for step in nextSteps:\n",
    "            x = names.index(name)\n",
    "            y = names.index(step)\n",
    "\n",
    "            probability = nextSteps[step] / sumOfSteps\n",
    "\n",
    "            transitionMatrix[x, y] = probability\n",
    "    \n",
    "    return transitionMatrix\n",
    "\n",
    "transitionMatrix = getTransitionMatrix()\n",
    "\n",
    "def predictNext(current):\n",
    "    \n",
    "    nextEvent = np.random.choice(a = names, p = transitionMatrix[names.index(current)])\n",
    "    \n",
    "    return nextEventnames = df_markov['concept:name'].unique().tolist() + ['editor: close_case']\n",
    "shape = len(names)\n",
    "\n",
    "transitionMatrix = np.zeros((shape, shape))\n",
    "\n",
    "def getTransitionMatrix():\n",
    "\n",
    "    for name in names:\n",
    "        nextSteps = creating_dict_for_next_step_stats(df_markov, name)[0]\n",
    "        sumOfSteps = sum(nextSteps.values())\n",
    "\n",
    "        for step in nextSteps:\n",
    "            x = names.index(name)\n",
    "            y = names.index(step)\n",
    "\n",
    "            probability = nextSteps[step] / sumOfSteps\n",
    "\n",
    "            transitionMatrix[x, y] = probability\n",
    "    \n",
    "    return transitionMatrix\n",
    "\n",
    "transitionMatrix = getTransitionMatrix()\n",
    "\n",
    "def predictNext(current):\n",
    "    \n",
    "    nextEvent = np.random.choice(a = names, p = transitionMatrix[names.index(current)])\n",
    "    \n",
    "    return nextEvent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_markovPrediction = df_markov.copy()\n",
    "\n",
    "nextEvents = []\n",
    "\n",
    "for i in tqdm(range(df_markovPrediction.shape[0])):\n",
    "    nextEvents.append(predictNext(df_markovPrediction['concept:name'][i]))\n",
    "\n",
    "df_markovPrediction['Prediction'] = nextEvents\n",
    "\n",
    "#Prepare the next event column\n",
    "df_markovPrediction['next_event'] = df_markovPrediction['concept:name']\n",
    "df_markovPrediction.loc[df_markovPrediction['step_number'] == 0, 'next_event'] = 'editor: close_case'\n",
    "df_markovPrediction['next_event'] = df_markovPrediction['next_event'].shift(-1)\n",
    "df_markovPrediction.loc[len(df) - 1, 'next_event'] = 'editor: close_case'\n",
    "\n",
    "df_markovPrediction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from random import randint\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group pre-cluster df by case (case_id)\n",
    "precluster_df = prepare_column_for_clustering(df['case_id'], df['concept:name'], df['concept:name'].unique())\n",
    "precluster_df_by_case = precluster_df.groupby('case_id').sum()\n",
    "precluster_df_by_case.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform PCA (2 component) on the data\n",
    "pca2d = PCA(n_components = 2)\n",
    "pca2d.fit(precluster_df_by_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform PCA (3 component) on the data\n",
    "pca3d = PCA(n_components = 3)\n",
    "pca3d.fit(precluster_df_by_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create 2d dataframe\n",
    "pca2d_df = pd.DataFrame(pca2d.transform(precluster_df_by_case))\n",
    "pca2d_df.index.names = ['case_id']\n",
    "pca2d_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create 3d dataframe\n",
    "pca3d_df = pd.DataFrame(pca3d.transform(precluster_df_by_case))\n",
    "pca3d_df.index.names = ['case_id']\n",
    "pca3d_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve Clusters\n",
    "df['concept:name cluster'], cluster_model = get_clusters(precluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group regular df by case (case_id)\n",
    "df_by_case = df.groupby('case_id').mean()[['concept:name cluster']]\n",
    "df_by_case.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the dataframes for 2d\n",
    "vis2d_df = pd.merge(df_by_case, pca2d_df, on='case_id', how='left')\n",
    "vis2d_df.columns = ['cluster', 'x_comp', 'y_comp']\n",
    "vis2d_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the dataframes for 3d\n",
    "vis3d_df = pd.merge(df_by_case, pca3d_df, on='case_id', how='left')\n",
    "vis3d_df.columns = ['cluster', 'x_comp', 'y_comp', 'z_comp']\n",
    "vis3d_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot with low alpha\n",
    "sns.lmplot('x_comp', 'y_comp', data=vis2d_df, hue='cluster', fit_reg=False, height=8, aspect=2, scatter_kws={'alpha': 0.25});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot with high alpha\n",
    "sns.lmplot('x_comp', 'y_comp', data=vis2d_df, hue='cluster', fit_reg=False, height=8, aspect=2, scatter_kws={'alpha': 0.9});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_colorscale(n):\n",
    "    lst = []\n",
    "    scale = 0\n",
    "    initial = ['red','green','blue','orange','yellow','purple','black','pink','cyan','magenta']\n",
    "    for i in range(n):\n",
    "        if i < len(initial):\n",
    "            color = initial[i]\n",
    "        else:\n",
    "            color = \"#\" + \"%06x\" % randint(0, 0xFFFFFF)\n",
    "        lst.append((round(scale, 2), color))\n",
    "        scale += 1/(n-1)\n",
    "        \n",
    "    \n",
    "    return lst\n",
    "#[(0, \"red\"), (0.14, \"yellow\"), (0.29, \"purple\"), (0.43, \"green\"), (0.57, \"blue\"), (0.71, \"black\"), (0.86, \"orange\"), (1, \"pink\")]\n",
    "random_colorscale(4)\n",
    "random_colorscale(NR_OF_CLUSTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [(0, \"red\"), (0.14, \"yellow\"), (0.29, \"purple\"), (0.43, \"green\"), (0.57, \"blue\"), (0.71, \"black\"), (0.86, \"orange\"), (1, \"pink\")]\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=vis3d_df['x_comp'],\n",
    "    y=vis3d_df['y_comp'],\n",
    "    z=vis3d_df['z_comp'],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=4,\n",
    "        color=vis3d_df['cluster'],\n",
    "        colorscale=random_colorscale(14),\n",
    "        opacity=0.5\n",
    "    )\n",
    ")])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions (g)raveyard:\n",
    "\n",
    "If you can get rid of something, but are afraid this might actually turn out useful in future:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def iterating_expected_actions(df : pd.DataFrame, concept_name : str, n : int) -> list:\n",
    "    '''concept_name is the starting point (first action)\n",
    "    n is the length of the longest trace ever observed\n",
    "    It is stored in lonest_trace but for runtime reasons, use n so far\n",
    "    '''\n",
    "    \n",
    "    longest_trace = max(df['step_number']) #finding the longest trace in the database (nr of steps)\n",
    "    #note that we determine this AFTER deleting some rows with uncomplete steps. We should be running this on full df\n",
    "    \n",
    "    i = 0\n",
    "    actions = [concept_name] #Here is the list that will store all the subsequent actions the algorithm decices to perform\n",
    "    while (i < n): #terminate if we are exceeding the max number of steps\n",
    "        wow = creating_dict_for_next_step_stats(df, concept_name)[0] #list all possible options\n",
    "        concept_name = choosing_next_action(wow) #Choose the most commonly used option\n",
    "        \n",
    "        if(cycles_shortcut(actions, concept_name, n) != False): #Checks if we are stuck in a loop\n",
    "            print(\"we are stuck in a loop\")\n",
    "            return(cycles_shortcut(actions, concept_name, n))\n",
    "        \n",
    "        if(concept_name == 'editor: close_case'): #If it is the \"terminate\" option - terminate\n",
    "            break\n",
    "        actions.append(concept_name) #Add the action to the list\n",
    "        i += 1\n",
    "    \n",
    "    actions.append('editor: close_case')\n",
    "    #print('i = ', i, \"n = \", n)\n",
    "    \n",
    "    return(actions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
